{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Harminder's Knowledge Base","text":"Harminder's Knowledge Base <p>         This is a repository of my personal knowledge that I'm committed to updating whenever I find interesting information, code worth sharing, or any intellectual rabbit hole I go down. My primary interests at the moment are machine learning-based web automation solutions such as web scraping liquidation auctions and online marketplaces, Home Assistant, self-hosted services (Snipe-IT, osTicket), 3D printing and design, and applying robotics to real-world problems.         </p> <p>         I'm an undergraduate student at Columbia Basin College enrolled in the Software Development Bachelor of Applied Science (BAS) program.         </p> Connect with me on LinkedIn. GitHub Activity OSRS Tracker","tags":["machine learning","web development","object detection","knowledge base"]},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2024/11/03/week-7-discussion-on-cybercrime-policing/","title":"Week 7 Discussion on Cybercrime Policing","text":"","tags":["Cybercrime","Law Enforcement","Policing","SOC305","Criminal Justice"]},{"location":"blog/2024/11/03/week-7-discussion-on-cybercrime-policing/#week-7-discussion-on-cybercrime-policing","title":"Week 7 Discussion on Cybercrime Policing","text":"<p>Due: Nov 3 at 11:59pm Course: SOC 305 Cybercrime: A Sociological Perspective</p>","tags":["Cybercrime","Law Enforcement","Policing","SOC305","Criminal Justice"]},{"location":"blog/2024/11/03/week-7-discussion-on-cybercrime-policing/#introduction","title":"Introduction","text":"<p>In this week's discussion, we delve into the challenges faced by law enforcement agencies in policing cybercrime. Drawing insights from David Wall's video \"Policing Cybercrime,\" we explore the complexities of investigating and prosecuting cybercrimes in the digital age. As technology advances, so do the methods employed by cybercriminals, posing unique obstacles for traditional policing methods. Let's examine the key takeaways from the video and discuss the implications for law enforcement in combating cybercrime.</p>","tags":["Cybercrime","Law Enforcement","Policing","SOC305","Criminal Justice"]},{"location":"blog/2024/11/03/week-7-discussion-on-cybercrime-policing/#key-challenges-in-policing-cybercrime","title":"Key Challenges in Policing Cybercrime","text":"<p>Policing cybercrime presents a formidable challenge for traditional law enforcement agencies. Law enforcement policy and standard practices have evolved to address physical crimes. In contrast, digital cyber threats are constantly adapting, taking advantage of individuals in other jurisdictions where law enforcement may have to navigate complex legal frameworks to prosecute offenders.</p> <p>The anonymity provided by VPNs, Tor networks, and digital currencies makes it much harder to track down perpetrators of cybercrimes. With the large number of cybercrimes being committed, law enforcement agencies are often overwhelmed and under-resourced without the help of automation and machine learning tools to assist in identifying patterns and potential threats.</p> <p>Lastly, cybercrime often goes unreported due to the bias of the severity of the crime and victims feeling helpless or embarrassed. This underreporting leads to a shortage of resources to combat cybercrime effectively.</p> <p>Overall, global cooperation and the use of advanced technologies by every law enforcement agency are required to hold cybercriminals accountable.</p>","tags":["Cybercrime","Law Enforcement","Policing","SOC305","Criminal Justice"]},{"location":"blog/2023/11/14/building-an-indexing-pipeline-for-linkedin-skill-assessments-quizzes-repository/","title":"Building an Indexing Pipeline for LinkedIn Skill Assessments Quizzes Repository","text":"","tags":["Python","Whoosh","Indexing","LinkedIn","Skill Assessments","Quizzes"]},{"location":"blog/2023/11/14/building-an-indexing-pipeline-for-linkedin-skill-assessments-quizzes-repository/#building-an-indexing-pipeline-for-linkedin-skill-assessments-quizzes-repository","title":"Building an Indexing Pipeline for LinkedIn Skill Assessments Quizzes Repository","text":"<p>Creating an efficient indexing pipeline for the <code>linkedin-skill-assessments-quizzes</code> repository involves systematic cloning, data processing, indexing, and query service setup. This comprehensive guide will walk you through each step with detailed code snippets, leveraging the <code>Whoosh</code> library for indexing.</p>","tags":["Python","Whoosh","Indexing","LinkedIn","Skill Assessments","Quizzes"]},{"location":"blog/2023/11/14/building-an-indexing-pipeline-for-linkedin-skill-assessments-quizzes-repository/#pre-requisites","title":"Pre-requisites","text":"<p>You want to create a new directory and name it <code>linkedin_skill_assessments_quizzes</code>, you need to first open the command prompt in the current working directory. To do this, you can use the following command in the command prompt.</p> <p></p><pre><code>cd path_to_your_current_working_directory\n</code></pre> Replace <code>path_to_your_current_working_directory</code> with the actual path where you want to create the new directory.   <p>Alternatively, on Windows, you can open the command prompt in the current working directory by clicking on the address bar and typing <code>cmd</code>, and pressing enter.  </p> <p>Once you are in the desired working directory, create a new directory named linkedin_skill_assessments_quizzes by executing the following command:  </p> <pre><code>mkdir linkedin_skill_assessments_quizzes\n</code></pre> <p>Now navigate to this new directory by executing the following command:</p> <pre><code>cd linkedin_skill_assessments_quizzes\n</code></pre> <p>This is where you will be cloning the repository and creating the indexing pipeline.</p>","tags":["Python","Whoosh","Indexing","LinkedIn","Skill Assessments","Quizzes"]},{"location":"blog/2023/11/14/building-an-indexing-pipeline-for-linkedin-skill-assessments-quizzes-repository/#introduction","title":"Introduction","text":"<p>LinkedIn Skill Assessments Quizzes is a repository of quizzes for various skills. It contains MD files for each quiz. The repository is available on GitHub. The repository has over 27,400 stars and 13,600 forks. It is a popular repository that is used by many people to prepare for interviews and improve their skills.</p>","tags":["Python","Whoosh","Indexing","LinkedIn","Skill Assessments","Quizzes"]},{"location":"blog/2023/11/14/building-an-indexing-pipeline-for-linkedin-skill-assessments-quizzes-repository/#step-1-cloning-the-repository","title":"Step 1: Cloning the Repository","text":"<p>Start by cloning the repository to your local environment. This makes the content available for processing.</p> <p></p><pre><code>git clone https://github.com/Ebazhanov/linkedin-skill-assessments-quizzes.git\n</code></pre>","tags":["Python","Whoosh","Indexing","LinkedIn","Skill Assessments","Quizzes"]},{"location":"blog/2023/11/14/building-an-indexing-pipeline-for-linkedin-skill-assessments-quizzes-repository/#step-2-converting-the-md-files-to-json","title":"Step 2: Converting the MD Files to JSON","text":"<p>Processing the data involves parsing the MD files converting them to JSON format to extract the relevant information. The following code snippet demonstrates how to extract the question, answer, image, and options from the MD files and save them in a JSON file. This is required for indexing the data which we will cover in the next step.</p> <p>Add the following code to a file named <code>process_data.py</code> in the same directory where you cloned the repository.</p> <pre><code>import os\nimport json\nimport markdown2\nimport re\n\n# Get the markdown files directory\n\ncloned_repository_directory = r\"C:\\Users\\Harminder Nijjar\\Desktop\\blog\\kb-blog-portfolio-mkdocs-master\\scripts\\linkedin-skill-assessments-quizzes\"\n\n# Create a folder to store the JSON files\n\noutput_folder = os.path.join(cloned_repository_directory, \"json_output\")\n\n# Create the output folder if it doesn't exist\n\nos.makedirs(output_folder, exist_ok=True)\n\n# Create a list to store data for each MD file\n\ndata_for_each_md = []\n\n# Iterate through the Markdown files (\\*.md) in the current directory and its subdirectories\n\nfor root, dirs, files in os.walk(cloned_repository_directory):\n    for name in files:\n        if name.endswith(\".md\"): # Construct the full path to the Markdown file\n            md_file_path = os.path.join(root, name)\n\n            # Read the Markdown file\n            with open(md_file_path, \"r\", encoding=\"utf-8\") as md_file:\n                md_content = md_file.read()\n\n            # Split the content into sections for each question and answer\n            sections = re.split(r\"####\\s+Q\\d+\\.\", md_content)\n\n            # Remove the first empty section\n            sections.pop(0)\n\n            # Create a list to store questions and answers for this MD file\n            questions_and_answers = []\n\n            # Iterate through sections and extract questions and answers\n            for section in sections:\n                # Split the section into lines\n                lines = section.strip().split(\"\\n\")\n\n                # Extract the question\n                question = lines[0].strip()\n\n                # Extract the answers\n                answers = [line.strip() for line in lines[1:] if line.strip()]\n\n                # Create a dictionary for this question and answers\n                qa_dict = {\"question\": question, \"answers\": answers}\n\n                # Append to the list of questions and answers\n                questions_and_answers.append(qa_dict)\n\n            # Create a dictionary for this MD file\n            md_data = {\n                \"markdown_file\": name,\n                \"questions_and_answers\": questions_and_answers,\n            }\n\n            # Append to the list of data for each MD file\n            data_for_each_md.append(md_data)\n\n# Save JSON files in the output folder\n\nfor md_data in data_for_each_md:\n    json_file_name = os.path.splitext(md_data[\"markdown_file\"])[0] + \".json\"\n    json_file_path = os.path.join(output_folder, json_file_name)\n    with open(json_file_path, \"w\", encoding=\"utf-8\") as json_file:\n        json.dump(md_data, json_file, indent=4)\n\nprint(f\"JSON files created for each MD file in the '{output_folder}' folder.\")w\n</code></pre> <p></p>","tags":["Python","Whoosh","Indexing","LinkedIn","Skill Assessments","Quizzes"]},{"location":"blog/2023/11/14/building-an-indexing-pipeline-for-linkedin-skill-assessments-quizzes-repository/#step-3-indexing-the-data","title":"Step 3: Indexing the Data","text":"<p>After processing the data, you can index it to make it searchable. Indexing refers to the process of creating an index for the data. The following code snippet demonstrates how to index the data using the <code>Whoosh</code> library. This is how the indexing pipeline will work.</p> <p>Add the following code to a file named <code>indexing_pipeline.py</code> in the same directory where you cloned the repository.</p> <pre><code>import os\nimport json\nimport whoosh\nfrom whoosh.fields import TEXT, ID, Schema\nfrom whoosh.index import create_in\n\n# Define the directory where your processed JSON files are located\n\njson_files_directory = r\"C:\\Users\\Harminder Nijjar\\Desktop\\blog\\kb-blog-portfolio-mkdocs-master\\scripts\\linkedin-skill-assessments-quizzes\\json_output\"\n\n# Define the directory where you want to create the Whoosh index\n\nindex_directory = r\"C:\\Users\\Harminder Nijjar\\Desktop\\blog\\kb-blog-portfolio-mkdocs-master\\scripts\\linkedin-skill-assessments-quizzes\\index\"\n\n# Create the schema for the Whoosh index\n\nschema = Schema(\nmarkdown_file=ID(stored=True),\nquestion=TEXT(stored=True),\nanswers=TEXT(stored=True),\n)\n\n# Create the index directory if it doesn't exist\n\nos.makedirs(index_directory, exist_ok=True)\n\n# Create the Whoosh index\n\nindex = create_in(index_directory, schema)\n\n# Open the index writer\n\nwriter = index.writer()\n\n# Iterate through JSON files and add documents to the index\n\nfor json_file_name in os.listdir(json_files_directory):\n    if json_file_name.endswith(\".json\"):\n        json_file_path = os.path.join(json_files_directory, json_file_name)\n        with open(json_file_path, \"r\", encoding=\"utf-8\") as json_file:\n            json_data = json.load(json_file) # Extract 'question' and 'answers' from the JSON file\n            question = json_data.get(\"question\", \"\")\n            answers = json_data.get(\"answers\", []) # Combine 'question' and 'answers' into a single field for searching\n            content = f\"{question} {' '.join(answers)}\"\n            writer.add_document(\n                markdown_file=json_file_name,\n                question=content,\n                answers=answers, # Use the extracted 'answers' or an empty list if not present\n            )\n\n# Commit changes to the index\n\nwriter.commit()\n\nprint(\"Indexing completed.\")\n</code></pre> <p></p>","tags":["Python","Whoosh","Indexing","LinkedIn","Skill Assessments","Quizzes"]},{"location":"blog/2023/11/14/building-an-indexing-pipeline-for-linkedin-skill-assessments-quizzes-repository/#step-4-setting-up-the-query-service","title":"Step 4: Setting Up the Query Service","text":"<p>After indexing the data, you can set up a query service to search the index for a given search term. The following code snippet demonstrates how to set up a query service using the <code>Whoosh</code> library. This is how the query service will work.</p> <pre><code>import os\nimport json\nimport re\nfrom whoosh.index import create_in, open_dir\nfrom whoosh.fields import Schema, TEXT, ID\nfrom whoosh.qparser import MultifieldParser\nfrom whoosh.analysis import StemmingAnalyzer\n\n# Define the schema for the index\n\nschema = Schema(\nquestion=TEXT(stored=True, analyzer=StemmingAnalyzer()),\nanswer=TEXT(stored=True),\nimage=ID(stored=True),\noptions=TEXT(stored=True),\n)\n\ndef create_search_index(json_files_directory, index_dir):\n    if not os.path.exists(index_dir):\n        os.makedirs(index_dir)\n\n    index = create_in(index_dir, schema)\n    writer = index.writer()\n\n    for json_filename in os.listdir(json_files_directory):\n        json_file_path = os.path.join(json_files_directory, json_filename)\n        if json_file_path.endswith(\".json\"):\n            try:\n                with open(json_file_path, \"r\", encoding=\"utf-8\") as file:\n                    data = json.load(file)\n                    for question_data in data[\"questions_and_answers\"]:\n                        question_text = question_data[\"question\"]\n                        answer_text = \"\\n\".join(question_data[\"answers\"])\n                        image_id = data.get(\"image_id\")\n                        options = question_data.get(\"options\", \"\")\n                        writer.add_document(\n                            question=question_text,\n                            answer=answer_text,\n                            image=image_id,\n                            options=options,\n                        )\n            except Exception as e:\n                print(f\"Failed to process file {json_file_path}: {e}\")\n\n    writer.commit()\n    print(\"Indexing completed successfully.\")\n\ndef extract_correct_answer(answer_text):\n    # Use regular expression to find the portion with \"- [x]\"\n    match = re.search(r\"- \\[x\\].\\*\", answer_text)\n    if match:\n        return match.group()\n    return None\n\ndef search_index(query_str, index_dir):\n    try:\n        ix = open_dir(index_dir)\n        with ix.searcher() as searcher:\n            parser = MultifieldParser([\"question\", \"options\"], schema=ix.schema)\n            query = parser.parse(query_str)\n            results = searcher.search(query, limit=None)\n            print(f\"Search for '{query_str}' returned {len(results)} results.\")\n            return [\n                {\n                    \"question\": result[\"question\"],\n                    \"correct_answer\": extract_correct_answer(result[\"answer\"]),\n                    \"image\": result.get(\"image\"),\n                }\n                for result in results\n            ]\n    except Exception as e:\n        print(\"An error occurred during the search.\")\n        return []\n\nif __name__ == \"__main__\":\n    json_files_directory = r\"C:\\Users\\Harminder Nijjar\\Desktop\\blog\\kb-blog-portfolio-mkdocs-master\\scripts\\linkedin-skill-assessments-quizzes\\json_output\" # Replace with your JSON files directory path\n    index_dir = \"index\" # Replace with your index directory path\n\n    create_search_index(json_files_directory, index_dir)\n\n    original_string = \"Why would you use a virtual environment?\"  # Replace with your actual search term\n    # Remove the special characters from the original string\n    query_string = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", original_string)\n    query_string = query_string.lower()\n    query_string = query_string.strip()\n    search_results = search_index(query_string, index_dir)\n\n    if search_results:\n        for result in search_results:\n            print(f\"Question: {result['question']}\")\n            # Remove the \"- [x]\" portion from the answer\n            print(f\"Correct answer: {result['correct_answer'].replace('- [x] ', '')}\")\n            if result.get(\"image\"):\n                print(f\"Image: {result['image']}\")\n            print(\"\\n\")\n        print(f\"Search for '{original_string}' completed successfully.\")\n        print(f\"Found {len(search_results)} results.\")\n    else:\n        print(\"No results found.\")\n</code></pre> <p></p>","tags":["Python","Whoosh","Indexing","LinkedIn","Skill Assessments","Quizzes"]},{"location":"blog/2023/11/14/building-an-indexing-pipeline-for-linkedin-skill-assessments-quizzes-repository/#conclusion","title":"Conclusion","text":"<p>Creating an efficient indexing pipeline for the 'linkedin-skill-assessments-quizzes' repository involves systematic cloning, data processing, indexing, and query service setup. This comprehensive guide has walked you through each step with detailed code snippets, leveraging the <code>Whoosh</code> library for indexing. You should now be able to query the index and get the results. The script will print the question, answer, and image (if available) for each result. </p> <p>Since the data is indexed, you can easily search for a given term and get the results. This can be useful for finding the answers to specific questions or searching for a particular topic. You can also use the query service to create a web application that allows users to search the index and get the results.</p>","tags":["Python","Whoosh","Indexing","LinkedIn","Skill Assessments","Quizzes"]},{"location":"blog/2023/11/14/building-an-indexing-pipeline-for-linkedin-skill-assessments-quizzes-repository/#references","title":"References","text":"<ul> <li>Whoosh Documentation</li> </ul>","tags":["Python","Whoosh","Indexing","LinkedIn","Skill Assessments","Quizzes"]},{"location":"blog/2024/01/30/how-to-write-a-simple-woodcutting-script-using-dreambot-api-in-2024/","title":"How to Write a Simple Woodcutting Script Using DreamBot API in 2024","text":"","tags":["DreamBot","Java","Woodcutting","Scripting"]},{"location":"blog/2024/01/30/how-to-write-a-simple-woodcutting-script-using-dreambot-api-in-2024/#how-to-write-a-simple-woodcutting-script-using-dreambot-api-in-2024","title":"How to Write a Simple Woodcutting Script Using DreamBot API in 2024","text":"<p>In this tutorial, we will walk through the process of creating a simple woodcutting script using the DreamBot API. This script will allow your in-game character to autonomously chop trees, bank logs, and repeat this process indefinitely.</p>","tags":["DreamBot","Java","Woodcutting","Scripting"]},{"location":"blog/2024/01/30/how-to-write-a-simple-woodcutting-script-using-dreambot-api-in-2024/#prerequisites","title":"Prerequisites","text":"<p>Before we begin, ensure you have the following:</p> <ul> <li>An Integrated Development Environment (IDE) of your choice. We will be using IntelliJ IDEA in this guide.</li> <li>A clean project containing your script's Main class.</li> <li>Basic understanding of Java.</li> </ul>","tags":["DreamBot","Java","Woodcutting","Scripting"]},{"location":"blog/2024/01/30/how-to-write-a-simple-woodcutting-script-using-dreambot-api-in-2024/#setting-up-your-project","title":"Setting Up Your Project","text":"<p>First, you need to set up your development environment. If you need help with this, you can visit Setting Up Your Development Environment.</p> <p>Next, create a new project and define your script's Main class. For help with this, visit Running Your First Script.</p>","tags":["DreamBot","Java","Woodcutting","Scripting"]},{"location":"blog/2024/01/30/how-to-write-a-simple-woodcutting-script-using-dreambot-api-in-2024/#creating-a-woodcutting-script","title":"Creating a Woodcutting Script","text":"<p>Our woodcutting script will involve various tasks such as finding trees, chopping them, walking to the bank, and depositing logs. We will create different states to handle these tasks.</p> <pre><code>public enum State {\n    FINDING_TREE,\n    CHOPPING_TREE,\n    WALKING_TO_BANK,\n    BANKING,\n    USEBANK,\n    WALKING_TO_TREES\n}\n</code></pre> <p>Now, we will create a method within our Main class that returns our current state:</p> <pre><code>public State getState() {\n    if (Inventory.isFull() &amp;&amp; !BANK_AREA.contains(Players.getLocal())) {\n        return State.WALKING_TO_BANK;\n    }\n    if (!Inventory.isFull() &amp;&amp; !TREE_AREA.contains(Players.getLocal())) {\n        return State.WALKING_TO_TREES;\n    }\n    if (Inventory.isFull() &amp;&amp; BANK_AREA.contains(Players.getLocal())) {\n        return State.BANKING;\n    }\n    if (!Inventory.isFull() &amp;&amp; TREE_AREA.contains(Players.getLocal())) {\n        return State.FINDING_TREE;\n    }\n    return null;\n}\n</code></pre>","tags":["DreamBot","Java","Woodcutting","Scripting"]},{"location":"blog/2024/01/30/how-to-write-a-simple-woodcutting-script-using-dreambot-api-in-2024/#walking-to-the-bank","title":"Walking to the Bank","text":"<p>Define a method to handle the state of walking to the bank:</p> <pre><code>if (Inventory.isFull() &amp;&amp; !BANK_AREA.contains(Players.getLocal())) {\n    return State.WALKING_TO_BANK;\n}\n</code></pre> <p>Next, implement the logic for walking to the bank in your main loop:</p> <pre><code>switch (getState()) {\n    case WALKING_TO_BANK:\n        if (!LocalPlayer.isMoving()) {\n            BANK_AREA.getRandomTile().click();\n        }\n        break;\n    // Other cases\n}\n</code></pre>","tags":["DreamBot","Java","Woodcutting","Scripting"]},{"location":"blog/2024/01/30/how-to-write-a-simple-woodcutting-script-using-dreambot-api-in-2024/#banking","title":"Banking","text":"<p>Now, let's handle the banking state. We'll start by interacting with the bank booth:</p> <pre><code>if (!Bank.isOpen() &amp;&amp; !LocalPlayer.isMoving()) {\n    GameObjects.closest(\"Bank booth\").interact(\"Bank\");\n}\n</code></pre> <p>Next, deposit the logs into the bank and close the bank interface:</p> <pre><code>case BANKING:\n    Bank.depositAll(\"Logs\");\n    Time.sleepUntil(() -&gt; !Inventory.contains(\"Logs\"), 2000);\n    if (!Inventory.contains(\"Logs\")) {\n        Bank.close();\n    }\n    break;\n</code></pre>","tags":["DreamBot","Java","Woodcutting","Scripting"]},{"location":"blog/2024/01/30/how-to-write-a-simple-woodcutting-script-using-dreambot-api-in-2024/#walking-back-to-the-tree-area","title":"Walking Back to the Tree Area","text":"<p>To return to the tree area, we need to add a new state and corresponding logic:</p> <pre><code>if (!Inventory.isFull() &amp;&amp; !TREE_AREA.contains(Players.getLocal())) {\n    return State.WALKING_TO_TREES;\n}\n\ncase WALKING_TO_TREES:\n    if (!LocalPlayer.isMoving()) {\n        TREE_AREA.getRandomTile().click();\n    }\n    break;\n</code></pre>","tags":["DreamBot","Java","Woodcutting","Scripting"]},{"location":"blog/2024/01/30/how-to-write-a-simple-woodcutting-script-using-dreambot-api-in-2024/#finding-and-chopping-trees","title":"Finding and Chopping Trees","text":"<p>Finally, implement the code that finds and chops trees:</p> <pre><code>case FINDING_TREE:\n    GameObject tree = GameObjects.closest(t -&gt; t.getName().equals(\"Tree\"));\n    if (tree != null &amp;&amp; tree.interact(\"Chop down\")) {\n        Time.sleepUntil(LocalPlayer::isAnimating, 2000);\n    }\n    break;\n</code></pre>","tags":["DreamBot","Java","Woodcutting","Scripting"]},{"location":"blog/2024/01/30/how-to-write-a-simple-woodcutting-script-using-dreambot-api-in-2024/#wrapping-up","title":"Wrapping Up","text":"<p>That's it! You've now created a basic woodcutting script using the DreamBot API. This script will autonomously navigate your character to chop trees, store logs in the bank, and repeat the process. Happy scripting!</p>","tags":["DreamBot","Java","Woodcutting","Scripting"]},{"location":"blog/2023/12/31/downloading-teri-meri-doriyaann-using-python-and-beautifulsoup/","title":"Downloading Teri Meri Doriyaann using Python and BeautifulSoup","text":"","tags":["Python","BeautifulSoup","Teri Meri Doriyaann","Hindi Serials","Star Network"]},{"location":"blog/2023/12/31/downloading-teri-meri-doriyaann-using-python-and-beautifulsoup/#downloading-teri-meri-doriyaann-using-python-and-beautifulsoup","title":"Downloading Teri Meri Doriyaann using Python and BeautifulSoup","text":"","tags":["Python","BeautifulSoup","Teri Meri Doriyaann","Hindi Serials","Star Network"]},{"location":"blog/2023/12/31/downloading-teri-meri-doriyaann-using-python-and-beautifulsoup/#overview","title":"Overview","text":"<p>In today's streaming-dominated era, accessing specific international content like the Hindi serial \"Teri Meri Doriyaann\" can be challenging due to regional restrictions or subscription barriers. This blog delves into a Python-based solution to download episodes of \"Teri Meri Doriyaann\" from a website using BeautifulSoup and Selenium.</p>","tags":["Python","BeautifulSoup","Teri Meri Doriyaann","Hindi Serials","Star Network"]},{"location":"blog/2023/12/31/downloading-teri-meri-doriyaann-using-python-and-beautifulsoup/#disclaimer","title":"Disclaimer","text":"<p>Important Note: This tutorial is intended for educational purposes only. Downloading copyrighted material without the necessary authorization is illegal and violates many websites' terms of service. Please ensure you comply with all applicable laws and terms of service.</p>","tags":["Python","BeautifulSoup","Teri Meri Doriyaann","Hindi Serials","Star Network"]},{"location":"blog/2023/12/31/downloading-teri-meri-doriyaann-using-python-and-beautifulsoup/#prerequisites","title":"Prerequisites","text":"<ul> <li>A working knowledge of Python.</li> <li>Python environment set up on your machine.</li> <li>Basic understanding of HTML structures and web scraping concepts.</li> </ul>","tags":["Python","BeautifulSoup","Teri Meri Doriyaann","Hindi Serials","Star Network"]},{"location":"blog/2023/12/31/downloading-teri-meri-doriyaann-using-python-and-beautifulsoup/#setting-up-the-scraper","title":"Setting Up the Scraper","text":"<p>The script provided utilizes Python with the Selenium package for browser automation and BeautifulSoup for parsing HTML. Here\u2019s a step-by-step breakdown:</p>","tags":["Python","BeautifulSoup","Teri Meri Doriyaann","Hindi Serials","Star Network"]},{"location":"blog/2023/12/31/downloading-teri-meri-doriyaann-using-python-and-beautifulsoup/#setup-logging","title":"Setup Logging","text":"<p>The first step involves setting up logging to monitor the script's execution and troubleshoot any issues.</p> <pre><code>import logging\n# Setup Logging\n\ndef setup_logger():\nlogger = logging.getLogger(**name**)\nlogger.setLevel(logging.INFO)\n\n    file_handler = logging.FileHandler(\"teri-meri-doriyaann-downloader.log\", mode=\"a\")\n    log_format = logging.Formatter(\n        \"%(asctime)s - %(name)s - [%(levelname)s] [%(pathname)s:%(lineno)d] - %(message)s - [%(process)d:%(thread)d]\"\n    )\n    file_handler.setFormatter(log_format)\n    logger.addHandler(file_handler)\n\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(log_format)\n    logger.addHandler(console_handler)\n\n    return logger\n\nlogger = setup_logger()\n</code></pre>","tags":["Python","BeautifulSoup","Teri Meri Doriyaann","Hindi Serials","Star Network"]},{"location":"blog/2023/12/31/downloading-teri-meri-doriyaann-using-python-and-beautifulsoup/#selenium-automation-class","title":"Selenium Automation Class","text":"<p>Selenium simulates browser interactions. The <code>SeleniumAutomation</code> class contains methods for opening web pages, extracting video links, and managing browser tasks.</p> <pre><code>from selenium import webdriver\n\n    # Selenium Automation\n\n    class SeleniumAutomation:\n    def **init**(self, driver):\n    self.driver = driver\n\n        def open_target_page(self, url):\n            self.driver.get(url)\n            time.sleep(5)\n</code></pre>","tags":["Python","BeautifulSoup","Teri Meri Doriyaann","Hindi Serials","Star Network"]},{"location":"blog/2023/12/31/downloading-teri-meri-doriyaann-using-python-and-beautifulsoup/#extracting-video-links","title":"Extracting Video Links","text":"<p>The <code>extract_video_links</code> method in the <code>SeleniumAutomation</code> class is crucial. It navigates web pages and extracts video URLs.</p> <pre><code>    def extract_video_links(self):\n        results = {\"videos\": []}\n        try: # Current date in the desired format DD-Month-YYYY\n        current_date = datetime.datetime.now().strftime(\"%d-%B-%Y\")\n\n                    link_selector = f'//*[@id=\"content\"]/div[5]/article[1]/div[2]/span/h2/a'\n                    if WebDriverWait(self.driver, 10).until(\n                        EC.element_to_be_clickable((By.XPATH, link_selector))\n                    ):\n                        self.driver.find_element(By.XPATH, link_selector).click()\n                        time.sleep(30)  # Adjust the timing as needed\n\n                        first_video_player = \"/html/body/div[1]/div[2]/div/div/div[1]/div/article/div[3]/center/div/p[14]/a\"\n                        second_video_player = \"/html/body/div[1]/div[2]/div/div/div[1]/div/article/div[3]/center/div/p[12]/a\"\n\n                        for player in [first_video_player, second_video_player]:\n                            if WebDriverWait(self.driver, 10).until(\n                                EC.element_to_be_clickable((By.XPATH, player))\n                            ):\n                                self.driver.find_element(By.XPATH, player).click()\n                                time.sleep(10)  # Adjust the timing as needed\n                                # Switch to the new tab that contains the video player\n                                self.driver.switch_to.window(self.driver.window_handles[1])\n                                elements = self.driver.find_elements(By.CSS_SELECTOR, \"*\")\n                                for element in elements:\n                                    if element.tag_name == \"iframe\" and element.get_attribute(\"src\"):\n                                        logger.info(f\"Element: {element.get_attribute('outerHTML')}\")\n                                        try:\n                                            video_url = element.get_attribute(\"src\")\n                                        except Exception as e:\n                                            logger.error(f\"Error getting video URL: {e}\")\n                                            continue\n\n                                        self.driver.get(video_url)\n                                        elements = self.driver.find_elements(By.CSS_SELECTOR, \"*\")\n                                        for element in elements:\n                                            if element.tag_name == \"video\" and element.get_attribute(\"src\") and element.get_attribute(\"src\").endswith(\".mp4\"):\n                                                logger.info(f\"Element: {element.get_attribute('outerHTML')}\")\n                                                try:\n                                                    video_url = element.get_attribute(\"src\")\n                                                except Exception as e:\n                                                    logger.error(f\"Error getting video URL: {e}\")\n                                                    continue\n\n                                                logger.info(f\"Video URL: {video_url}\")\n                                                response = requests.get(video_url, stream=True)\n                                                with open(f\"E:\\\\Plex\\\\Teri Meri Doriyaann\\\\{datetime.datetime.now().strftime('%m-%d-%Y')}.mp4\", \"wb\") as f:\n                                                    for chunk in response.iter_content(chunk_size=1024*1024):\n                                                        logger.info(f\"Writing chunk {chunk}\")\n                                                        if chunk:\n                                                            f.write(chunk)\n                                                            logger.info(f\"Chunk {chunk} written\")\n                                                            break\n\n                except Exception as e:\n                    logger.error(f\"Error in extract_video_links: {e}\")\n\n            def close_browser(self):\n                self.driver.quit()\n</code></pre>","tags":["Python","BeautifulSoup","Teri Meri Doriyaann","Hindi Serials","Star Network"]},{"location":"blog/2023/12/31/downloading-teri-meri-doriyaann-using-python-and-beautifulsoup/#video-scraper-class","title":"Video Scraper Class","text":"<p><code>VideoScraper</code> manages the scraping process, from initializing the web driver to saving the extracted video links.</p> <pre><code>    # Video Scraper\n    class VideoScraper:\n    def **init**(self):\n    self.user = os.getlogin()\n    self.selenium = None\n\n        def setup_driver(self):\n            # Set up ChromeDriver service\n            service = Service()\n            options = webdriver.ChromeOptions()\n            options.add_argument(f\"--user-data-dir=C:\\\\Users\\\\{self.user}\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\")\n            options.add_argument(\"--profile-directory=Default\")\n            return webdriver.Chrome(service=service, options=options)\n\n        def start_scraping(self):\n            try:\n                self.selenium = SeleniumAutomation(self.setup_driver())\n                self.selenium.open_target_page(\"https://www.desi-serials.cc/watch-online/star-plus/teri-meri-doriyaann/\")\n                videos = self.selenium.extract_video_links()\n                self.save_videos(videos)\n            finally:\n                if self.selenium:\n                    self.selenium.close_browser()\n\n        def save_videos(self, videos):\n            with open(\"desi_serials_videos.json\", \"w\", encoding=\"utf-8\") as file:\n                json.dump(videos, file, ensure_ascii=False, indent=4)\n</code></pre>","tags":["Python","BeautifulSoup","Teri Meri Doriyaann","Hindi Serials","Star Network"]},{"location":"blog/2023/12/31/downloading-teri-meri-doriyaann-using-python-and-beautifulsoup/#running-the-scraper","title":"Running the Scraper","text":"<p>The script execution brings together all the components of the scraping process.</p> <pre><code>    if **name** == \"**main**\":\n        os.system(\"taskkill /im chrome.exe /f\")\n        scraper = VideoScraper()\n        scraper.start_scraping()\n</code></pre>","tags":["Python","BeautifulSoup","Teri Meri Doriyaann","Hindi Serials","Star Network"]},{"location":"blog/2023/12/31/downloading-teri-meri-doriyaann-using-python-and-beautifulsoup/#conclusion","title":"Conclusion","text":"<p>This script demonstrates using Python's web scraping capabilities for specific content access. It highlights the use of Selenium for browser automation and BeautifulSoup for HTML parsing. While focused on a specific TV show, the methodology is adaptable for various web scraping tasks.</p> <p>Use such scripts responsibly and within legal and ethical boundaries. Happy scraping and coding!</p>","tags":["Python","BeautifulSoup","Teri Meri Doriyaann","Hindi Serials","Star Network"]},{"location":"blog/2023/12/31/downloading-teri-meri-doriyaann-using-python-and-beautifulsoup/#references","title":"References","text":"<ul> <li>GitHub Repository</li> </ul>","tags":["Python","BeautifulSoup","Teri Meri Doriyaann","Hindi Serials","Star Network"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/","title":"Transferring Script Files to Local System or VPS","text":"","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#transferring-script-files-to-local-system-or-vps","title":"Transferring Script Files to Local System or VPS","text":"","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#local-system-setup-process-windows","title":"Local System Setup Process (Windows)","text":"<p>This document outlines the process for transferring a Python script and setting it up on your local system. The script, in this case, is a Facebook Marketplace Scraper that allows you to collect and manage data from online listings.</p>","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with the setup, ensure you have the following prerequisites ready:</p> <ul> <li>Python installed on your system (Python 3.6 or higher is recommended).</li> <li>Access to a Google Cloud project with required credentials for Google Sheets API.</li> <li>SQLite database support.</li> <li>A Telegram bot token (if you wish to receive notifications).</li> <li>Dependencies listed in the <code>requirements.txt</code> file provided with the script.</li> </ul>","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#setup-steps","title":"Setup Steps","text":"","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#step-1-obtain-script-files","title":"Step 1: Obtain Script Files","text":"<p>1.1. Obtain the necessary script files from your source, typically provided as a ZIP archive or downloadable files. 1.2. Ensure you have the following script files:</p> <ul> <li><code>fb_parser.py</code>: The main Python script.</li> <li><code>requirements.txt</code>: A file containing the required Python dependencies.</li> </ul>","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#step-2-install-dependencies","title":"Step 2: Install Dependencies","text":"<p>2.1. Open a terminal/command prompt and navigate to the directory containing the script files. 2.2. Install the required Python dependencies using the following command: </p><pre><code>pip install -r requirements.txt\n</code></pre> This command installs packages such as <code>requests</code>, <code>beautifulsoup4</code>, and others.","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#step-3-configure-credentials","title":"Step 3: Configure Credentials","text":"<p>3.1. Set up Google Cloud credentials for accessing the Google Sheets API:</p> <ul> <li>Create or use an existing Google Cloud project.</li> <li>Enable the Google Sheets API for your project.</li> <li>Create OAuth 2.0 credentials for a desktop application and download the <code>credentials.json</code> file.</li> <li>Place the <code>credentials.json</code> file in the same directory as the script.</li> </ul>","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#step-4-initialize-the-database","title":"Step 4: Initialize the Database","text":"<p>4.1. Initialize the SQLite database by running the following command in the script's directory: </p><pre><code>python fb_parser.py --initdb\n</code></pre> <p>This command creates the SQLite database file (<code>market_listings.db</code>) in the script's directory.</p>","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#step-5-configure-telegram-bot-token-optional","title":"Step 5: Configure Telegram Bot Token (Optional)","text":"<p>5.1. If you want to receive notifications via Telegram, edit the <code>fb_parser.py</code> script and update the <code>bot_token</code> and <code>bot_chat_id</code> variables with your own values.</p>","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#step-6-run-the-scraper","title":"Step 6: Run the Scraper","text":"<p>6.1. Start the scraper by running the following command in the script's directory: </p><pre><code>python fb_parser.py\n</code></pre> <p>The scraper will begin collecting data from Facebook Marketplace listings, and notifications will be sent if configured.</p>","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#step-7-monitor-and-review","title":"Step 7: Monitor and Review","text":"<p>7.1. Monitor the script's output for any messages or errors. 7.2. Review the Google Sheets document to ensure that it's collecting data accurately.</p>","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#step-8-ongoing-management","title":"Step 8: Ongoing Management","text":"<p>8.1. Consider setting up automated scheduling, if required, to run the scraper at specific intervals.</p>","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#vps-setup-process","title":"VPS Setup Process","text":"","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#overview","title":"Overview","text":"<p>This document outlines the process for transferring a Python script and setting it up on your VPS (Virtual Private Server). The script, in this case, is a Facebook Marketplace Scraper designed to collect and manage data from online listings.</p>","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#prerequisites_1","title":"Prerequisites","text":"<p>Before proceeding with the setup, ensure you have the following prerequisites ready:</p> <ol> <li> <p>Access to a VPS: You should have access to a VPS with administrative privileges. You can obtain VPS services from providers like AWS, DigitalOcean, or any other preferred hosting provider.</p> </li> <li> <p>Operating System: The VPS should be running a compatible operating system, preferably a Linux distribution such as Ubuntu or CentOS.</p> </li> <li> <p>Python Installed: Python 3.6 or higher should be installed on your VPS. You can check the installed Python version using the <code>python3 --version</code> command.</p> </li> <li> <p>Access to SSH: Ensure you can access your VPS via SSH (Secure Shell) with a terminal or SSH client.</p> </li> <li> <p>Script Files: Obtain the necessary script files for the Facebook Marketplace Scraper. These files are typically provided as a ZIP archive or downloadable files.</p> </li> <li> <p>Dependencies: Review the script's documentation to identify and install any required Python dependencies.</p> </li> </ol>","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#setup-steps_1","title":"Setup Steps","text":"","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#step-1-access-your-vps","title":"Step 1: Access Your VPS","text":"<ul> <li>Log in to your VPS using SSH. You should have received SSH credentials from your hosting provider. <pre><code>ssh username@hostname\n</code></pre> Replace <code>username</code> with your VPS username and <code>your-vps-ip</code> with the actual IP address or hostname of your VPS.</li> </ul>","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#step-2-upload-script-files","title":"Step 2: Upload Script Files","text":"<ul> <li>Transfer the necessary script files to your VPS. You can use secure file transfer methods like SCP or SFTP to upload files from your local machine to the VPS.</li> </ul>","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#step-3-install-python-dependencies","title":"Step 3: Install Python Dependencies","text":"<ul> <li>Install the required Python dependencies on your VPS. Use the package manager appropriate for your Linux distribution. For example, on Ubuntu, you can use <code>apt-get</code>: <pre><code>sudo apt-get update\nsudo apt-get install python3-pip\npip3 install -r requirements.txt\n</code></pre> Replace <code>requirements.txt</code> with the actual filename containing the dependencies.</li> </ul>","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#step-4-configure-credentials","title":"Step 4: Configure Credentials","text":"<ul> <li>Set up any necessary credentials for the script. This may include configuring API keys, OAuth tokens, or other authentication details required for your specific use case.</li> </ul>","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#google-sheets-api","title":"Google Sheets API","text":"<ol> <li>Go to the Google Cloud Console.</li> <li>Create a new project if you don't have one.</li> <li>In the project dashboard, navigate to \"APIs &amp; Services\" &gt; \"Credentials.\"</li> <li>Click on \"Create credentials\" and choose \"OAuth client ID.\"</li> <li>Configure the OAuth consent screen with the necessary details.</li> <li>Select \"Desktop App\" as the application type.</li> <li>Create the OAuth client ID.</li> <li>Download the JSON credentials file (usually named credentials.json).</li> </ol>","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#telegram-bot-api-chat-id","title":"Telegram Bot API (Chat ID)","text":"<ol> <li>Message the parser bot on Telegram.</li> <li>Navigate to the following URL in your browser: <pre><code>https://api.telegram.org/bot&lt;yourtoken&gt;/getUpdates\n</code></pre> Replace <code>&lt;yourtoken&gt;</code> with your bot's token.</li> <li>Look for the \"chat\" object in the response. The \"id\" value is your chat ID.</li> </ol>","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#step-5-execute-the-script","title":"Step 5: Execute the Script","text":"<ul> <li> <p>Run the Python script on your VPS. Navigate to the directory where you uploaded the script files and execute it. </p><pre><code>python3 fb_parser.py\n</code></pre> Replace <code>fb_parser.py</code> with the actual filename of the script. </li> <li> <p>Monitor the script's output for any messages or errors. Depending on your VPS setup, you may choose to run the script in the background using tools like <code>nohup</code> or within a screen session for detached operation.</p> </li> </ul>","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#step-6-ongoing-management","title":"Step 6: Ongoing Management","text":"<ul> <li>Consider setting up automated scheduling, if required, to run the scraper at specific intervals. You can use tools like <code>cron</code> for scheduling periodic tasks on your VPS.</li> </ul>","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#conclusion","title":"Conclusion","text":"<p>Transferring script files to your local system or VPS to set up a Facebook Marketplace Scraper is a straightforward process. By following the steps outlined in this document, you can quickly get started with the scraper and begin collecting data from online listings.</p>","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/27/transferring-script-files-to-local-system-or-vps/#references","title":"References","text":"<ul> <li>Facebook Marketplace Scraper</li> <li>Google Sheets API</li> <li>SQLite</li> <li>Telegram Bot API</li> <li>Python</li> <li>Smartproxy</li> </ul>","tags":["Facebook Marketplace Scraper","Python","Google Sheets API","SQLite","Telegram Bot API","Smartproxy"]},{"location":"blog/2023/11/07/openais-developer-conference/","title":"OpenAI's Developer Conference","text":"","tags":["OpenAI","GPT4-Turbo","AI","Multimodal","Assistants API","GPT Store","Microsoft","Azure"]},{"location":"blog/2023/11/07/openais-developer-conference/#openais-developer-conference-a-new-era-of-ai-innovation","title":"OpenAI's Developer Conference: A New Era of AI Innovation","text":"","tags":["OpenAI","GPT4-Turbo","AI","Multimodal","Assistants API","GPT Store","Microsoft","Azure"]},{"location":"blog/2023/11/07/openais-developer-conference/#gpt-4-turbo-with-128k-context-breaking-boundaries-in-language-modeling","title":"GPT-4 Turbo with 128K context: Breaking Boundaries in Language Modeling","text":"<p>OpenAI announced GPT4-Turbo at its November Developer Conference, a new language model that builds on the success of GPT-3. This model is designed to break boundaries in language modeling, offering increased context length, more control, better knowledge, new modalities, customization, and higher rate limits. As shown, GPT-4 Turbo offers a significant increase in the number of tokens it can handle in its context length, going from 8,000 tokens to 128,000 tokens. This represents a substantial enhancement in the model's ability to maintain context over longer conversations or documents. Compared to the standard GPT-4, this is a huge leap forward in terms of the amount of information that can be processed by the model.</p> <p>The new model also offers more control, specifically in terms of model inputs and outputs, and better knowledge, which includes updating the cut-off date for knowledge about the world to April 2023 and providing the ability for developers to easily add their own knowledge base. New modalities, such as DALL-E 3, Vision, and TTS (text-to-speech) will all be included in the API, with a new version of Whisper speech recognition coming. Customization, including fine-tuning and custom models (which, Altman warned, won\u2019t be cheap), and higher rate limits are also included in the new model, making it a comprehensive upgrade over its predecessors.</p> <p> </p>","tags":["OpenAI","GPT4-Turbo","AI","Multimodal","Assistants API","GPT Store","Microsoft","Azure"]},{"location":"blog/2023/11/07/openais-developer-conference/#multimodal-capabilities-expanding-ais-horizon","title":"Multimodal Capabilities: Expanding AI's Horizon","text":"","tags":["OpenAI","GPT4-Turbo","AI","Multimodal","Assistants API","GPT Store","Microsoft","Azure"]},{"location":"blog/2023/11/07/openais-developer-conference/#gpt-4-turbo-with-vision","title":"GPT-4 Turbo with vision","text":"<p>GPT-4 now integrates vision, allowing it to understand and analyze images, enhancing its capabilities beyond text. Developers can utilize this feature through the gpt-4-vision-preview model. It supports a range of applications, including caption generation and detailed image analysis, beneficial for services like BeMyEyes, which aids visually impaired individuals. The vision feature will soon be included in GPT-4's stable release. Costs vary by image size; for example, a 1080\u00d71080 image analysis costs approximately $0.00765. For more details, OpenAI provides a comprehensive vision guide and DALL\u00b7E 3 remains the tool for image generation.</p> GPT-4 Turbo with vision analyzing Old School RuneScape through the RuneLite interface <pre><code>import base64\nimport logging\nimport os\nimport time\nfrom PIL import ImageGrab, Image\nimport pyautogui as gui\nimport pygetwindow as gw\nimport requests\n\n# Set up logging to capture events when script runs and any possible errors.\n\nlog_filename = 'rune_capture.log' # Replace with your desired log file name\nlogging.basicConfig(\nfilename=log_filename,\nfilemode='a',\nlevel=logging.INFO,\nformat='%(asctime)s - %(name)s - [%(levelname)s] [%(pathname)s:%(lineno)d] - %(message)s - [%(process)d:%(thread)d]'\n)\nlogger = logging.getLogger(**name**)\n\n# Set the client window title.\n\nclient_window_title = \"RuneLite\"\n\ndef capture_screenshot():\ntry: # Get the title of the client window.\nwin = gw.getWindowsWithTitle(client_window_title)[0]\nwin.activate()\ntime.sleep(1)\n\n        # Get the client window's position.\n        clientWindow = gw.getWindowsWithTitle(client_window_title)[0]\n        x1, y1 = clientWindow.topleft\n        x2, y2 = clientWindow.bottomright\n\n        # Define the screenshot path and crop area.\n        path = \"gameWindow.png\"\n        gui.screenshot(path)\n        img = Image.open(path)\n        img = img.crop((x1 + 1, y1 + 40, x2 - 250, y2 - 165))\n        img.save(path)\n        return path\n\n    except Exception as e:\n        logger.error(f\"An error occurred while capturing screenshot: {e}\")\n        raise\n\ndef encode_image(image_path):\ntry:\nwith open(image_path, \"rb\") as image_file:\nreturn base64.b64encode(image_file.read()).decode(\"utf-8\")\nexcept Exception as e:\nlogger.error(f\"An error occurred while encoding image: {e}\")\nraise\n\ndef send_image_to_api(base64_image):\napi_key = os.getenv(\"OPENAI_API_KEY\")\nheaders = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {api_key}\"}\n\n    payload = {\n        \"model\": \"gpt-4-vision-preview\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What\u2019s in this image?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}]},\n        ],\n        \"max_tokens\": 300,\n    }\n\n    try:\n        response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n        response.raise_for_status()  # Will raise an exception for HTTP errors.\n        return response.json()\n\n    except Exception as e:\n        logger.error(f\"An error occurred while sending image to API: {e}\")\n        raise\n\nif **name** == \"**main**\":\n  try: # Perform the main operations.\n      screenshot_path = capture_screenshot()\n      base64_image = encode_image(screenshot_path)\n      api_response = send_image_to_api(base64_image)\n      print(api_response)\n  except Exception as e:\n      logger.error(f\"An error occurred in the main function: {e}\")\n</code></pre>","tags":["OpenAI","GPT4-Turbo","AI","Multimodal","Assistants API","GPT Store","Microsoft","Azure"]},{"location":"blog/2023/11/07/openais-developer-conference/#dalle-3","title":"DALL\u00b7E 3","text":"<p>Developers can now access DALL\u00b7E 3, a multimodal model that generates images from text directly through the API by specifying <code>dall-e-3</code> as the model. </p>","tags":["OpenAI","GPT4-Turbo","AI","Multimodal","Assistants API","GPT Store","Microsoft","Azure"]},{"location":"blog/2023/11/07/openais-developer-conference/#tts-text-to-speech","title":"TTS (Text-to-Speech)","text":"<p>OpenAI's newest model is available to generate human-quality speech from text via their text-to-speech API. </p>","tags":["OpenAI","GPT4-Turbo","AI","Multimodal","Assistants API","GPT Store","Microsoft","Azure"]},{"location":"blog/2023/11/07/openais-developer-conference/#revenue-sharing-gpt-store-empowering-creators","title":"Revenue-Sharing GPT Store: Empowering Creators","text":"<p>The DevDay also cast a spotlight on the newly announced revenue-sharing GPT Store. This platform represents a strategic move towards a more inclusive creator economy within AI, offering compensation to creators of AI applications based on user engagement and usage. This initiative is a nod to the growing importance of content creators in the AI ecosystem and reflects a broader trend of recognizing and rewarding the contributions of individual developers and innovators.</p>","tags":["OpenAI","GPT4-Turbo","AI","Multimodal","Assistants API","GPT Store","Microsoft","Azure"]},{"location":"blog/2023/11/07/openais-developer-conference/#microsoft-partnership-and-azures-role","title":"Microsoft Partnership and Azure's Role","text":"<p>The ongoing collaboration with Microsoft was highlighted, with a focus on how Azure's infrastructure is being optimized to support OpenAI's sophisticated AI models. This partnership is a testament to the shared goal of accelerating AI innovation and enhancing integration across various services and platforms as well as Microsoft's heavy investment in AI.</p>","tags":["OpenAI","GPT4-Turbo","AI","Multimodal","Assistants API","GPT Store","Microsoft","Azure"]},{"location":"blog/2023/11/07/openais-developer-conference/#safe-and-gradual-ai-integration","title":"Safe and Gradual AI Integration","text":"<p>OpenAI emphasized a strategic approach to AI integration, advocating for a balance between innovation and safety. The organization invites developers to engage with the new tools thoughtfully, ensuring a responsible progression of AI within different sectors. This measured approach is a reflection of OpenAI's commitment to the safe and ethical development of AI technologies. </p>","tags":["OpenAI","GPT4-Turbo","AI","Multimodal","Assistants API","GPT Store","Microsoft","Azure"]},{"location":"blog/2023/11/07/openais-developer-conference/#conclusion","title":"Conclusion","text":"<p>The Developer Conference marked a notable milestone for OpenAI and the broader AI community. The launch of GPT4-Turbo and the introduction of new multimodal capabilities, combined with the support of Microsoft's Azure and the innovative revenue-sharing model of the GPT Store, heralds a new phase of growth and experimentation in AI applications.</p>","tags":["OpenAI","GPT4-Turbo","AI","Multimodal","Assistants API","GPT Store","Microsoft","Azure"]},{"location":"blog/2024/10/09/qidi-plus-4-bed-mesh-correction-process/","title":"QIDI Plus 4 Bed Mesh Correction Process","text":"","tags":["QIDI Plus 4","3D Printing","Fluidd","ABL","Calibration"]},{"location":"blog/2024/10/09/qidi-plus-4-bed-mesh-correction-process/#qidi-plus-4-bed-mesh-correction-process","title":"QIDI Plus 4 Bed Mesh Correction Process","text":"","tags":["QIDI Plus 4","3D Printing","Fluidd","ABL","Calibration"]},{"location":"blog/2024/10/09/qidi-plus-4-bed-mesh-correction-process/#introduction","title":"Introduction","text":"<p>Upgrading from the Ender 3 Pro to the QIDI Plus 4 was an exciting step forward, but it introduced me to a new component of 3D printing technology. Having never used Fluidd or Automatic Bed Leveling (ABL) before, I knew I had to experiment. The transition from manually leveling a print bed to utilizing these advanced tools posed a change, albeit a welcomed one. After several calibration attempts, adjustments, and refinements, I achieved an acceptable variance in the range of the bed mesh. In this article, I\u2019ll walk you through the step-by-step process that transformed my bed mesh from highly uneven to leveled.</p>","tags":["QIDI Plus 4","3D Printing","Fluidd","ABL","Calibration"]},{"location":"blog/2024/10/09/qidi-plus-4-bed-mesh-correction-process/#the-initial-bed-mesh-reading","title":"The Initial Bed Mesh Reading","text":"<p>Upon starting the first calibration with Fluidd, the bed mesh data was clear\u2014the bed was far from level. The range between the highest and lowest points was 4.5341, with the lower end at -2.6816 and the highest at 2.0525. This much variance was causing severe print issues, including poor adhesion and inconsistent first layers.</p> <p></p>","tags":["QIDI Plus 4","3D Printing","Fluidd","ABL","Calibration"]},{"location":"blog/2024/10/09/qidi-plus-4-bed-mesh-correction-process/#step-1-first-adjustments-and-hex-nut-corrections","title":"Step 1: First Adjustments and Hex Nut Corrections","text":"<p>I started by focusing on the front right and back right corners, the highest points on the bed. My first instinct was to loosen these hex nuts to bring the bed down in these areas. After running the ABL calibration, the mesh improved slightly, but the range was still significant, increasing slightly to 4.6741.</p> <p></p> <p>Noticing the small improvement but the persistent issue, I realized I needed to focus on the back left corner, which was too low. I tightened this hex nut to raise that section of the bed.</p>","tags":["QIDI Plus 4","3D Printing","Fluidd","ABL","Calibration"]},{"location":"blog/2024/10/09/qidi-plus-4-bed-mesh-correction-process/#step-2-incremental-tightening-and-loosening","title":"Step 2: Incremental Tightening and Loosening","text":"<p>With small adjustments to the hex nuts, I saw a real difference. Using a methodical approach, I turned each hex nut 25 degrees at a time:</p> <ul> <li>I tightened the back right nut by four 25-degree turns to bring down the higher side.</li> <li>I then loosened the back left hex nut by two 25-degree turns to raise the lower corner.</li> </ul> <p>These incremental adjustments began to close the gap, reducing the range between the high and low points and creating a more even bed. The bed mesh range was now at 1.5309\u2014a significant improvement from where I started.</p> <p></p>","tags":["QIDI Plus 4","3D Printing","Fluidd","ABL","Calibration"]},{"location":"blog/2024/10/09/qidi-plus-4-bed-mesh-correction-process/#step-3-fine-tuning-the-bed-level","title":"Step 3: Fine-Tuning the Bed Level","text":"<p>After each adjustment, I recalibrated the bed using Fluidd\u2019s automatic bed leveling tool. The mesh had become much more balanced, but there was still room for improvement. I continued making small changes:</p> <ul> <li>I tightened the front left hex nut slightly to lower the high points.</li> <li>I continued loosening the back left hex nut to gradually raise the back left edge.</li> </ul> <p>After each adjustment, I recalibrated and checked the bed mesh results to see how the bed was leveling out.</p>","tags":["QIDI Plus 4","3D Printing","Fluidd","ABL","Calibration"]},{"location":"blog/2024/10/09/qidi-plus-4-bed-mesh-correction-process/#step-4-achieving-a-bed-mesh-range-under-05","title":"Step 4: Achieving a Bed Mesh Range Under 0.5","text":"<p>After multiple rounds of precise tightening and loosening, the bed mesh finally reached a balanced state with a range of 0.3913. The highest point on the bed was 1.1743, while the lowest was -0.5825. This marked a significant improvement from where I started, bringing the bed to an acceptable level. With a range under 0.5, the bed was now flat enough to provide a stable surface for consistent, high-quality prints.</p> <p></p>","tags":["QIDI Plus 4","3D Printing","Fluidd","ABL","Calibration"]},{"location":"blog/2024/10/09/qidi-plus-4-bed-mesh-correction-process/#conclusion","title":"Conclusion","text":"<p>By methodically tightening the hex nuts using a socket wrench with a 15 mm hex socket\u2014R for tightening and L for loosening\u2014and utilizing Fluidd\u2019s automatic bed leveling tool to calibrate and check the bed mesh, I was able to greatly improve the levelness of my print bed. Achieving a balanced mesh allows for a consistent first layer, solving many of the adhesion and printing issues I had encountered earlier. Though the process requires time and attention, fine-tuning the bed level is essential for successful prints when using ABL technology. With patience and persistence, anyone can achieve a perfectly leveled bed mesh on their 3D printer.</p>","tags":["QIDI Plus 4","3D Printing","Fluidd","ABL","Calibration"]},{"location":"blog/2024/11/04/qidi-plus-4-winter-redemption-arc--removing-back-panel-and-ssr-check-update/","title":"QIDI Plus 4 Winter Redemption Arc \u2013 Removing Back Panel and SSR Check Update","text":"","tags":["QIDI Plus 4","3D Printing","Printer Troubleshooting","Product Review","Community Update"]},{"location":"blog/2024/11/04/qidi-plus-4-winter-redemption-arc--removing-back-panel-and-ssr-check-update/#qidi-plus-4-winter-redemption-arc-removing-back-panel-and-ssr-check-update","title":"QIDI Plus 4 Winter Redemption Arc \u2013 Removing Back Panel and SSR Check Update","text":"","tags":["QIDI Plus 4","3D Printing","Printer Troubleshooting","Product Review","Community Update"]},{"location":"blog/2024/11/04/qidi-plus-4-winter-redemption-arc--removing-back-panel-and-ssr-check-update/#introduction","title":"Introduction","text":"<p>Yesterday I heard back from QIDI Tech support regarding the issues I've been facing with my QIDI Plus 4 3D printer. I want to start by thanking everyone who responded to my post yesterday regardeless of the tone of the response. I appreciate the concern and harmony in the community. I also want to apologize for any confusion caused by my previous posts. I understand that my initial posts may have caused some alarm, and I want to clarify that my intention was to share my experience and raise awareness about potential safety concerns with the QIDI Plus 4. I did not intend to spread fear or misinformation, and as such I have removed by previous post and also provided images of the back panel removal and SSR check process.</p>","tags":["QIDI Plus 4","3D Printing","Printer Troubleshooting","Product Review","Community Update"]},{"location":"blog/2024/11/04/qidi-plus-4-winter-redemption-arc--removing-back-panel-and-ssr-check-update/#the-back-panel-removal-process","title":"The Back Panel Removal Process","text":"<p>I removed the back panel of the QIDI Plus 4 to access the Solid State Relay (SSR) board and inspect it for any signs of damage or overheating. The process was straightforward, requiring only a few tools and careful handling to avoid damaging any components. Here are the steps I followed to remove the back panel:</p> <ol> <li>Power Off and Unplug: Before starting, I powered off the printer and unplugged it from the power source to ensure safety.</li> <li>Remove the Screws: Using an Allen key, I removed the screws holding the back panel in place. </li> <li>Gently Pull the Panel: With the screws removed, I gently pulled the back panel away from the printer to expose the internal components.</li> <li>Inspect the SSR Board: Once the panel was removed, I carefully inspected the main board for any signs of discoloration, burning, or damage.</li> <li>Remove SSR Cover and Check: I removed the SSR cover and checked the SSR board for any visible signs of overheating or damage.</li> <li>Reassemble the Printer: After inspecting the SSR board, I reassembled the printer by carefully replacing the SSR cover, back panel and securing it with their respective screws.</li> </ol>","tags":["QIDI Plus 4","3D Printing","Printer Troubleshooting","Product Review","Community Update"]},{"location":"blog/2024/11/04/qidi-plus-4-winter-redemption-arc--removing-back-panel-and-ssr-check-update/#the-ssr-check-results","title":"The SSR Check Results","text":"<p>After inspecting the SSR board, I found no visible signs of damage or discoloration. However there is still a lingering smell of burnt plastic which I have reported to QIDI Tech support back on October 11, 2024 however, I'm not able to determine which component is causing the smell. I will keep you updated on any further developments.  </p>","tags":["QIDI Plus 4","3D Printing","Printer Troubleshooting","Product Review","Community Update"]},{"location":"blog/2024/11/04/qidi-plus-4-winter-redemption-arc--removing-back-panel-and-ssr-check-update/#conclusion","title":"Conclusion","text":"<p>I want to thank the community for their support and understanding as I navigate these issues with my QIDI Plus 4. I will continue to provide updates on my progress and any further actions I take to address the safety concerns and usability issues with the printer. </p>","tags":["QIDI Plus 4","3D Printing","Printer Troubleshooting","Product Review","Community Update"]},{"location":"blog/2023/11/16/progress-log---runescapegpt/","title":"Progress Log - RunescapeGPT","text":"","tags":["RunescapeGPT","AI","Language Model","Transformer","OpenAI","GPT"]},{"location":"blog/2023/11/16/progress-log---runescapegpt/#progress-update-runescapegpt-a-runescape-chatgpt-bot","title":"Progress Update: RunescapeGPT - A Runescape ChatGPT Bot","text":"","tags":["RunescapeGPT","AI","Language Model","Transformer","OpenAI","GPT"]},{"location":"blog/2023/11/16/progress-log---runescapegpt/#introduction","title":"Introduction","text":"<p>RunescapeGPT is a project I started in order to create an AI-powered color bot for Runescape with enhanced capabilities. I have been working on this project for a few days now, and I am excited to share my progress with you all. In this post, I will be discussing what I have done so far and what I plan to do next.</p>","tags":["RunescapeGPT","AI","Language Model","Transformer","OpenAI","GPT"]},{"location":"blog/2023/11/16/progress-log---runescapegpt/#what-ive-done-so-far","title":"What I've Done So Far","text":"","tags":["RunescapeGPT","AI","Language Model","Transformer","OpenAI","GPT"]},{"location":"blog/2023/11/16/progress-log---runescapegpt/#2021-11-16","title":"2021-11-16","text":"<p>I have created a GUI for the bot using Qt Creator. It is a simple GUI that is inspired by Sammich's AHK bot. It has all the buttons provided by Sammich's bot.</p> <p>Here is a screenshot of Sammich's GUI:</p> <p></p> <p>And here is the current state of RunescapeGPT's GUI:</p> <p></p> <p>Although the GUI is not fully functional yet, it lays a solid foundation. The next steps in development include adding actionable functionality to the buttons. Initially, we'll start with a single script that has a hotkey to send a screenshot to the AI model. This will be a key feature for monitoring the bot's activity and ensuring its smooth operation.</p> <p>The script will capture the current state of the game, including what the bot is doing at any given time, and send this information along with a screenshot to the AI model. This multimodal approach will allow the AI to analyze both the textual data and the visual context of the game, enabling it to make informed decisions about the bot's next actions.</p>","tags":["RunescapeGPT","AI","Language Model","Transformer","OpenAI","GPT"]},{"location":"blog/2023/11/16/progress-log---runescapegpt/#upcoming-features-and-enhancements","title":"Upcoming Features and Enhancements","text":"<ul> <li>Real-time Monitoring: Integrate a system to always have a variable that reflects the bot's current action.</li> <li>Activity Log and Reporting: Keep a detailed log of the bot's last movement, including timestamps and the duration between actions, to identify and understand if something unusual occurs.</li> <li>AI-Powered Decision Making: In the event of anomalies or breaks, the information, including the screenshot, will be sent to an AI model equipped with multimodal capabilities. This model will analyze the situation and guide the bot accordingly.</li> </ul> <p>By implementing these features, RunescapeGPT will become more than just a bot; it will be a sophisticated AI companion that navigates the game's challenges with unprecedented efficiency.</p> <p>Stay tuned for more updates as the project evolves!</p>","tags":["RunescapeGPT","AI","Language Model","Transformer","OpenAI","GPT"]},{"location":"blog/2024/11/02/time-discussion-board/","title":"Time Discussion Board","text":"","tags":["Project Management","Planning Fallacy","CPM","Time Estimation","PMBOK"]},{"location":"blog/2024/11/02/time-discussion-board/#time-discussion-board-assignment","title":"Time Discussion Board Assignment","text":"<p>Due: Nov 1 at 11:59pm Course: PROJ100 45520 - F24 - Intro to Project Mgmt</p>","tags":["Project Management","Planning Fallacy","CPM","Time Estimation","PMBOK"]},{"location":"blog/2024/11/02/time-discussion-board/#describe-the-importance-of-timeschedule-planning-and-monitoring-in-at-least-1-paragraph","title":"Describe the importance of time/schedule planning and monitoring in at least 1 paragraph","text":"<p>Time and schedule planning are important to ensuring project deadlines are met in their respective timeframes. Time and schedule planning minimize delays and cost overruns. According to Horine(2022), resource allocation and task prioritization are key outcomes of effective schedule planning. Furthermore, the PMBOK guide details how monitoring the schedule is crucial for early risk determination and mitigating them before they hinder the project's progress (PMI, 2017).</p>","tags":["Project Management","Planning Fallacy","CPM","Time Estimation","PMBOK"]},{"location":"blog/2024/11/02/time-discussion-board/#describe-how-the-critical-path-method-cpm-works-and-how-it-impacts-the-final-milestone-in-at-least-1-paragraph","title":"Describe how the critical path method (CPM) works and how it impacts the final milestone in at least 1 paragraph","text":"<p>The Critical Path Method (CPM) is a schedule network analysis technique that estimates the minimum project duration and determines the amount of schedule flexibility on the logical network paths within the schedule model (A Guide to the Project Management Body of Knowledge [PMBOK\u00ae Guide], 2017, p. 210). Not taking any resource limitations into account, the CPM calculates early start, early finish, late start, and late finish dates for each activity in the project. The critical path is the longest path through a project, and it determines the shortest possible project duration. The CPM impacts the final milestone by identifying which activities must be managed closely to ensure the project is completed on time.</p>","tags":["Project Management","Planning Fallacy","CPM","Time Estimation","PMBOK"]},{"location":"blog/2024/11/02/time-discussion-board/#explain-activity-logic-and-float-in-at-least-1-paragraph","title":"Explain activity logic and float in at least 1 paragraph","text":"<p>Activity logic is the sequence and relationship between tasks in a project's schedule. It aligns the tasks in precedence order to ensure that certain tasks with prerequisites are only started once the preceding tasks are finished. Activity logic plays an important role in figuring out the critical path, the longest path through a project's activities, and the shortest possible project duration (Horine, 2022). Float or slack is the amount of time a task can be delayed before it affects the overall timeline or the start of any dependent tasks.</p>","tags":["Project Management","Planning Fallacy","CPM","Time Estimation","PMBOK"]},{"location":"blog/2024/11/02/time-discussion-board/#if-you-cannot-build-a-detailed-schedule-what-other-methods-can-you-use-to-manage-the-project-timeline-identify-at-least-two","title":"If you cannot build a detailed schedule, what other methods can you use to manage the project timeline?  Identify at least two","text":"<p>Two alternative methods for managing the project timeline without a detailed schedule are milestone charts and Kanban boards. Both are helpful in providing a visual representation of the project timeline and the tasks that need to be completed.</p>","tags":["Project Management","Planning Fallacy","CPM","Time Estimation","PMBOK"]},{"location":"blog/2024/11/02/time-discussion-board/#describe-the-planning-fallacy-from-your-article-search-and-describe-one-of-the-ways-to-mitigate-it-in-at-least-1-paragraph","title":"Describe the Planning Fallacy from your article search and describe one of the ways to mitigate it in at least 1 paragraph","text":"<p>The Planning Fallacy is a cognitive bias that causes individuals to underestimate task durations and can hinder project timelines due to overly optimistic predictions. As Yamini and Marathe (2018) explain, people often assume tasks will follow best-case scenarios despite evidence suggesting otherwise. This bias frequently leads to procrastination and project delays, impacting the project\u2019s schedule. One way to mitigate the Planning Fallacy is by implementing threshold-based incentives, particularly within supply chain management. Such incentives encourage employees to begin tasks early by rewarding them for time saved before a deadline. This proactive approach reduces procrastination and aligns task completion with more realistic time estimates, supporting effective schedule planning and reducing the risk of delays (Yamini &amp; Marathe, 2018).</p>","tags":["Project Management","Planning Fallacy","CPM","Time Estimation","PMBOK"]},{"location":"blog/2024/11/02/time-discussion-board/#references","title":"References","text":"<ul> <li> <p>Horine, G.M., (2022). Project Management Absolute Beginner\u2019s Guide. Que. Fifth Edition.</p> </li> <li> <p>Project Management Institute. (2017). A Guide to the Project Management Body of Knowledge (PMBOK\u00ae Guide) \u2013 Sixth Edition and The Standard for Project Management (ENGLISH): Vol. Sixth edition. Project Management Institute.</p> </li> <li> <p>Yamini, S., &amp; Marathe, R. R. (2018). Mathematical model to mitigate planning fallacy and to determine realistic delivery time. IIMB Management Review (Elsevier Science), 30(3), 242\u2013257. https://doi-org.columbiabasin.idm.oclc.org/10.1016/j.iimb.2018.05.003</p> </li> </ul>","tags":["Project Management","Planning Fallacy","CPM","Time Estimation","PMBOK"]},{"location":"blog/2024/11/03/review-of-sources-for-management-investigation-report/","title":"Review of Sources for Management Investigation Report","text":"","tags":["Management Report","Literature Review","Communication Models","Research Synthesis"]},{"location":"blog/2024/11/03/review-of-sources-for-management-investigation-report/#review-of-sources-for-management-investigation-report-on-implementing-effective-management-practices-for-home-media-servers","title":"Review of Sources for Management Investigation Report on Implementing Effective Management Practices for Home Media Servers","text":"","tags":["Management Report","Literature Review","Communication Models","Research Synthesis"]},{"location":"blog/2024/11/03/review-of-sources-for-management-investigation-report/#overview-of-home-media-server-management","title":"Overview of Home Media Server Management","text":"<p>When setting up a home media server, two aspects of the device which will be managing the media server are crucial: the power usage and processing power. The latest Raspberry Pi single-board computer, Raspberry Pi 5, is two to three times faster than its predecessors, with a power consumption of 25 watts (Kofler, 2024).  </p> <p> (KL, 2024)</p> <p> Its computing power, small size and low price make the Raspberry Pi SBC an ideal choice for projects, including building an at-home media service that provides on-demand access to movies, music, and other media content over the internet (KL, 2024). Popular media-streaming software like Kodi, Plex, Emby and Jellyfin can be used to stream your media content to various devices, including smart TVs, streaming boxes such as Apple TV, Chromecast and Amazon Fire TV, and mobile devices (Parkyn, 2018).</p>","tags":["Management Report","Literature Review","Communication Models","Research Synthesis"]},{"location":"blog/2024/11/03/review-of-sources-for-management-investigation-report/#open-source-media-server-software","title":"Open Source Media Server Software","text":"<p>After Kodi became associated with piracy, Plex emerged as a popular alternative for managing media content (Parkyn, 2018). However, we've seen key features in Plex being locked behind a paywall. Jellyfin follows the same server-client model as Plex which allows for streaming content over your network but with an added focus on privacy and open-source software (Peers, 2022). Not only that, Jellyfin can easily be run on a Raspberry Pi 3 or later models (Peers, 2020).</p>","tags":["Management Report","Literature Review","Communication Models","Research Synthesis"]},{"location":"blog/2024/11/03/review-of-sources-for-management-investigation-report/#media-management-and-automation","title":"Media Management and Automation","text":"<p>Tools like Radarr, Sonarr, Prowlarr and QBittorrent can help with automating media managment on your server. By implementing these tools, you can automate the process of downloading, organizing, and streaming media content to your devices. Radarr and Sonarr are used for managing movies and TV shows, respectively, while Prowlarr acts as a proxy for accessing private torrent trackers. QBittorrent is a lightweight torrent client that can be used to download media content from various sources (Gupta, 2022).</p>","tags":["Management Report","Literature Review","Communication Models","Research Synthesis"]},{"location":"blog/2024/11/03/review-of-sources-for-management-investigation-report/#conclusion","title":"Conclusion","text":"<p>The Raspberry Pi 5 is more than capable of running Jellyfin and with services like Radarr, Sonarr, Prowlarr and QBittorrent, you can automate the process of managing your media content. This creates an experience like Over The Top (OTT) third-party services such as Netflix and Hulu, but with the added benefit of privacy and control over your media library.</p>","tags":["Management Report","Literature Review","Communication Models","Research Synthesis"]},{"location":"blog/2024/11/03/qidi-plus-4-issues-and-request-for-replacement-or-refund/","title":"QIDI Plus 4 Issues and Request for Replacement or Refund","text":"","tags":["QIDI Plus 4","3D Printing","Product Review","Printer Issues","Safety"]},{"location":"blog/2024/11/03/qidi-plus-4-issues-and-request-for-replacement-or-refund/#qidi-plus-4-ssr-issues-and-request-for-replacement-or-refund","title":"QIDI Plus 4 SSR Issues and Request for Replacement or Refund","text":"","tags":["QIDI Plus 4","3D Printing","Product Review","Printer Issues","Safety"]},{"location":"blog/2024/11/03/qidi-plus-4-issues-and-request-for-replacement-or-refund/#introduction","title":"Introduction","text":"<p>I purchased the QIDI Plus 4 3D printer from their official website on September 24, 2024 and received it on September 30, 2024. I was excited to upgrade from my Ender 3 Pro and explore the new features and capabilities of the QIDI Plus 4. I've printed primarily PLA with some PETG and TPU on the Ender 3 Pro and was looking forward to the enhanced printing experience with the QIDI Plus 4.However on October 11, 2024, I encountered a burning smell coming from the printer which seemed to be originating from underneath the poop duct. I immediately turned off the printer and unplugged it to prevent any further damage. I contacted QIDI Tech support and heard back from them a few days later. </p> <p></p> <p> In my eagerness and wanting to get back to printing, I side-stepped the issue and continued printing letting support know that the smell had dissipated. However, on November 1, the Plus 4 ran into an issue with the extruder not heating at all. I contacted support again have been waiting for a response since then.</p>","tags":["QIDI Plus 4","3D Printing","Product Review","Printer Issues","Safety"]},{"location":"blog/2024/11/03/qidi-plus-4-issues-and-request-for-replacement-or-refund/#the-primary-issue","title":"The Primary Issue","text":"","tags":["QIDI Plus 4","3D Printing","Product Review","Printer Issues","Safety"]},{"location":"blog/2024/11/03/qidi-plus-4-issues-and-request-for-replacement-or-refund/#burning-smell","title":"Burning Smell","text":"<p>The burning smell issue and potential safety risks associated with the QIDI Plus 4 have raised significant concerns. After investigating online forums and user-reported issues, I discovered a PSA that highlighted serious safety flaws with the printer\u2019s Solid State Relay (SSR) board responsible for the chamber heater. A video posted on YouTube by a concerned user, along with a stream by Grant from 3D Musketeers, indicated that the chamber heater and SSR board were prone to overheating due to design flaws and potential voltage mismatches. </p> <p> Specifically, users reported the SSR board drawing more power than its rated capacity, which in some cases exceeded 500 watts, causing discoloration and burning on components around the heater\u2019s coils. These findings suggest that the printer could pose a fire hazard if left unaddressed, especially for users on 120-volt systems where incorrect configurations reportedly result in excess current through the SSR board\u200b</p>","tags":["QIDI Plus 4","3D Printing","Product Review","Printer Issues","Safety"]},{"location":"blog/2024/11/03/qidi-plus-4-issues-and-request-for-replacement-or-refund/#secondary-issue","title":"Secondary Issue","text":"","tags":["QIDI Plus 4","3D Printing","Product Review","Printer Issues","Safety"]},{"location":"blog/2024/11/03/qidi-plus-4-issues-and-request-for-replacement-or-refund/#extruder-heating-failure","title":"Extruder Heating Failure","text":"<p>The extruder heating failure on November 1, 2024, further compounds the safety concerns and usability issues with the QIDI Plus 4. The inability to heat the extruder prevents the printer from functioning correctly, rendering it unusable for its intended purpose. The sudden failure of a critical component like the extruder raises questions about the printer\u2019s reliability and long-term performance. Given the safety risks associated with the SSR board and the extruder heating failure, it is evident that the QIDI Plus 4 has significant design or manufacturing flaws that need to be addressed promptly.</p>","tags":["QIDI Plus 4","3D Printing","Product Review","Printer Issues","Safety"]},{"location":"blog/2024/11/03/qidi-plus-4-issues-and-request-for-replacement-or-refund/#the-request","title":"The Request","text":"","tags":["QIDI Plus 4","3D Printing","Product Review","Printer Issues","Safety"]},{"location":"blog/2024/11/03/qidi-plus-4-issues-and-request-for-replacement-or-refund/#replacement-or-refund","title":"Replacement or Refund","text":"<p>Given the safety concerns and the printer\u2019s inability to function properly, I am going to be requesting a replacement or a refund for the QIDI Plus 4. The issues encountered, including the burning smell and extruder heating failure, indicate a fundamental flaw in the printer\u2019s design or manufacturing process. As a customer, I expect a reliable and safe product that meets the advertised specifications and quality standards. The safety risks associated with the SSR board and the potential for fire hazards are unacceptable, and I cannot continue using the printer in its current state.</p>","tags":["QIDI Plus 4","3D Printing","Product Review","Printer Issues","Safety"]},{"location":"blog/2024/11/03/summary-and-critical-evaluation-of-obstacles-to-cybercrime-investigations/","title":"Summary and Critical Evaluation of \"Obstacles to Cybercrime Investigations\"","text":"","tags":["Cybercrime","Cybercrime Investigations","Law Enforcement Challenges","UNODC","Cybersecurity"]},{"location":"blog/2024/11/03/summary-and-critical-evaluation-of-obstacles-to-cybercrime-investigations/#summary-and-critical-evaluation-of-obstacles-to-cybercrime-investigations","title":"Summary and Critical Evaluation of \"Obstacles to Cybercrime Investigations\"","text":"<p>UNODC (United Nations Office on Drugs and Crime) published an article titled \"Obstacles to Cybercrime Investigations,\" which delves into the challenges faced by law enforcement agencies in investigating and prosecuting cybercrimes. This summary and critical evaluation aim to provide an overview of the key points discussed in the article, analyze its implications, and offer a critical perspective on the effectiveness of current investigative practices.</p>","tags":["Cybercrime","Cybercrime Investigations","Law Enforcement Challenges","UNODC","Cybersecurity"]},{"location":"blog/2024/11/03/summary-and-critical-evaluation-of-obstacles-to-cybercrime-investigations/#summary","title":"Summary","text":"<p>Among the various obstacles authorities must face when investigating cybercrimes, the article highlights the following key challenges:</p> <ol> <li> <p>Anonymity and Anonymization Techniques: Cybercriminals exploit anonymity provided by legitimate tools like proxy servers, The Onion Router (Tor), and anonymized IP addresses to conceal their identities and activities. They're also able to host websites on the dark web allowing like-minded individuals with malicious intent to share information and tools for cybercrimes while remaining hidden.</p> </li> <li> <p>Attribution and Traceback: Determining who is responsible for cybercrime is another challenge that is made even more difficult when cybercriminals use malware-infected devices or botnets to commit crimes. Back-tracing illicit acts to their source is time-consuming and resource-intensive, especially when perpetrators use anonymization techniques to hide their identities.</p> </li> <li> <p>Legal and Evidentiary Challenges: National and international legal frameworks often have stringent communication and cooperation requirements for sharing digital evidence and information across borders. The lack of harmonized cybercrime laws and mutual legal assistance agreements hinders effective investigations and prosecutions.</p> </li> </ol>","tags":["Cybercrime","Cybercrime Investigations","Law Enforcement Challenges","UNODC","Cybersecurity"]},{"location":"blog/2024/11/03/summary-and-critical-evaluation-of-obstacles-to-cybercrime-investigations/#critical-evaluation","title":"Critical Evaluation","text":"<p>Given that one of the primary obstacles encountered by law enforcement and government agencies is \"brain drain\" or the loss of skilled cybercrime investigators to the private sector, private tech companies and cybersecurity firms should have a legal obligation to contribute to the public good by sharing resources and tools alongside providing training to law enforcement agencies. This would help bridge the gap between the public and private sectors, enhancing the overall capacity of law enforcement to combat cybercrime effectively. A better-trained workforce will be more willing and able to tackle the challenges posed by cybercriminals.</p> <p>Adversary nations often exploit the digital infrastructure of other countries to launch cyberattacks due to the difficulty of pinpointing who the perpetrator of a cybercrime is. Training lawmakers to understand the basics of cybersecurity would be beneficial in creating more effective legislation for fighting cybercrime. This would also help in creating a more secure digital environment for citizens and businesses.</p>","tags":["Cybercrime","Cybercrime Investigations","Law Enforcement Challenges","UNODC","Cybersecurity"]},{"location":"blog/2024/11/03/summary-and-critical-evaluation-of-obstacles-to-cybercrime-investigations/#conclusion","title":"Conclusion","text":"<p>The article \"Obstacles to Cybercrime Investigations\" provides a comprehensive overview of the challenges faced by law enforcement agencies in investigating cybercrimes. By addressing the issues of anonymity, attribution, legal frameworks, and the need for enhanced cooperation between public and private sectors, authorities can better equip themselves to combat cybercriminal activities effectively. The critical evaluation suggests that a multi-faceted approach involving training, resource sharing, and legislative reforms is essential to overcome the obstacles and strengthen the investigative capabilities of law enforcement agencies in the digital age.</p>","tags":["Cybercrime","Cybercrime Investigations","Law Enforcement Challenges","UNODC","Cybersecurity"]},{"location":"blog/2023/11/05/osrs-money-making-guide-2024---how-to-earn-a-free-runescape-bond/","title":"OSRS Money Making Guide 2024 - How to Earn a Free RuneScape Bond","text":"","tags":["OSRS","RuneScape","Money Making","Guide","Bond"]},{"location":"blog/2023/11/05/osrs-money-making-guide-2024---how-to-earn-a-free-runescape-bond/#osrs-money-making-guide-2024-how-to-earn-a-free-runescape-bond","title":"OSRS Money Making Guide 2024 - How to Earn a Free RuneScape Bond","text":"<p>Help the OSRS community and earn a bond!</p> <p>One Small Wiki Favour is a project to improve the Old School RuneScape Wiki, where wiki users can contribute to a number of ongoing wiki tasks and qualify for receiving in-game rewards, including bonds.</p> <p>Anyone is welcome to participate, whether you're completely new to the wiki or you've been here for a while. If you have not edited before, then this is a great time to get involved!</p> <p>To participate in this project, join the #oswf-osrs channel in OSRS Wiki's Discord to coordinate with other editors, discuss the project, and claim your rewards.</p>","tags":["OSRS","RuneScape","Money Making","Guide","Bond"]},{"location":"blog/2023/11/05/osrs-money-making-guide-2024---how-to-earn-a-free-runescape-bond/#rules","title":"Rules","text":"<ul> <li>You must have a wiki account and cannot be blocked. Anonymous edits will not count. You can sign up for a wiki account at Special:CreateAccount. </li> <li>The reward for completion of a task will be one bond or equivalent coins. If multiple editors contribute to a goal then the prize may be split between them.</li> <li>Partial rewards may be awarded for completing part of a task, with a minimum award at 25% of a bond.</li> <li>A task must be completed in its entirety before any bonds or partial bonds are awarded for that task.</li> <li>If you are awarded a bond or a share of a bond, you can get in contact with one of the project runners on Discord, and claim it in-game.</li> <li>You can claim a maximum of two bonds over each two week period that tasks are active.</li> </ul>","tags":["OSRS","RuneScape","Money Making","Guide","Bond"]},{"location":"blog/2023/11/05/osrs-money-making-guide-2024---how-to-earn-a-free-runescape-bond/#automatic-notifications","title":"Automatic notifications","text":"<p>To get automatic notifications of new tasks, you can join the  Disord and subscribe to the <code>#oswf-osrs</code> channel.</p> <p>Alternatively, you can build an IFTTT applet using the RSS feed of the recent changes to the wiki. You can then automatically generate a Notion page with the new tasks and have them sent to you via text message or email.</p>","tags":["OSRS","RuneScape","Money Making","Guide","Bond"]},{"location":"blog/2023/12/04/automating-dvr-surveillance-feed-analysis-using-selenium-and-python/","title":"Automating DVR Surveillance Feed Analysis Using Selenium and Python","text":"","tags":["Selenium","Python","Computer Vision","Surveillance","DVR"]},{"location":"blog/2023/12/04/automating-dvr-surveillance-feed-analysis-using-selenium-and-python/#automating-dvr-surveillance-feed-analysis-using-selenium-and-python","title":"Automating DVR Surveillance Feed Analysis Using Selenium and Python","text":"","tags":["Selenium","Python","Computer Vision","Surveillance","DVR"]},{"location":"blog/2023/12/04/automating-dvr-surveillance-feed-analysis-using-selenium-and-python/#introduction","title":"Introduction","text":"<p>In an era where security and monitoring are paramount, leveraging technology to enhance surveillance systems is crucial. Our mission is to automate the process of capturing surveillance feeds from a DVR system for analysis using advanced computer vision techniques. This task addresses the challenge of accessing live video feeds from DVRs that do not readily provide direct stream URLs, such as RTSP, which are essential for real-time video analysis.</p>","tags":["Selenium","Python","Computer Vision","Surveillance","DVR"]},{"location":"blog/2023/12/04/automating-dvr-surveillance-feed-analysis-using-selenium-and-python/#the-challenge","title":"The Challenge","text":"<p>Many DVR (Digital Video Recorder) systems, especially older models or those using proprietary software, do not offer an easy way to access their video feeds for external processing. They often stream video through embedded ActiveX controls in web interfaces, which pose a significant barrier to automation due to their closed nature and security restrictions.</p>","tags":["Selenium","Python","Computer Vision","Surveillance","DVR"]},{"location":"blog/2023/12/04/automating-dvr-surveillance-feed-analysis-using-selenium-and-python/#our-approach","title":"Our Approach","text":"<p>To overcome these challenges, we propose a method that automates a web browser to periodically capture screenshots of the DVR's camera screens. These screenshots can then be analyzed using a computer vision model to transcribe or interpret the activities captured by the cameras. Our tools of choice are Selenium, a powerful tool for automating web browsers, and Python, a versatile programming language with extensive support for image processing and machine learning.</p>","tags":["Selenium","Python","Computer Vision","Surveillance","DVR"]},{"location":"blog/2023/12/04/automating-dvr-surveillance-feed-analysis-using-selenium-and-python/#step-by-step-guide","title":"Step-by-Step Guide","text":"<ul> <li>Setting Up the Environment    Selenium WebDriver: Install Selenium WebDriver compatible with your intended browser.    Python Environment: Set up a Python environment with the necessary libraries (selenium, datetime, etc.).</li> <li>Browser Automation    Navigate to DVR Interface: Use Selenium to open the browser and navigate to the DVR's web interface.    Handle Authentication: Automate the login process to access the camera feeds.</li> <li>Capturing Screenshots    Regular Intervals: Implement a loop in Python to capture and save screenshots of the camera feed every five seconds.    Timestamped Filenames: Save the screenshots with timestamps to ensure uniqueness and facilitate chronological analysis.</li> <li>Analyzing the Captured Screenshots    Vision Model Selection: Choose a suitable computer vision model for analyzing the screenshots based on the required analysis (e.g., object detection, and movement tracking).    Processing Screenshots: Feed the screenshots to the vision model either in real-time or in batches for analysis.</li> <li>Continuous Monitoring    Long-term Operation: Ensure the script can run continuously to monitor the surveillance feed over extended periods.</li> <li>Error Handling: Implement robust error handling to manage     browser timeouts, disconnections, or other potential issues.</li> </ul>","tags":["Selenium","Python","Computer Vision","Surveillance","DVR"]},{"location":"blog/2023/12/04/automating-dvr-surveillance-feed-analysis-using-selenium-and-python/#purpose-and-benefits","title":"Purpose and Benefits","text":"<p>This automated approach is designed to enhance surveillance systems where direct access to video streams is not available. By analyzing the DVR feeds, it can be used for various applications such as:</p> <p>Security Monitoring: Detect unauthorized activities or security breaches. Data Analysis: Gather data over time for pattern recognition or anomaly detection. Event Documentation: Keep a record of events with timestamps for future reference.</p>","tags":["Selenium","Python","Computer Vision","Surveillance","DVR"]},{"location":"blog/2023/12/04/automating-dvr-surveillance-feed-analysis-using-selenium-and-python/#conclusion","title":"Conclusion","text":"<p>While this approach offers a workaround to the limitations of certain DVR systems, it highlights the potential of integrating modern technology with existing surveillance infrastructure. The combination of Selenium's web automation capabilities and Python's powerful data processing and machine learning libraries opens up new avenues for enhancing security and surveillance systems.</p>","tags":["Selenium","Python","Computer Vision","Surveillance","DVR"]},{"location":"blog/2023/12/04/automating-dvr-surveillance-feed-analysis-using-selenium-and-python/#important-note","title":"Important Note","text":"<p>This method, while innovative, is a workaround and has limitations compared to direct video stream access. It is suited for scenarios where no other direct methods are available and real-time processing is not a critical requirement.</p>","tags":["Selenium","Python","Computer Vision","Surveillance","DVR"]},{"location":"blog/2023/11/27/hosting-mkdocs-documentation-on-github-pages/","title":"Hosting MkDocs Documentation on GitHub Pages","text":"","tags":["MkDocs","GitHub Pages","Documentation","Static Site Generator","Python"]},{"location":"blog/2023/11/27/hosting-mkdocs-documentation-on-github-pages/#hosting-mkdocs-documentation-on-github-pages","title":"Hosting MkDocs Documentation on GitHub Pages","text":"<p>This guide will walk you through the process of hosting your MkDocs documentation on GitHub Pages. By following these steps, you can make your documentation accessible online and easily share it with others.</p>","tags":["MkDocs","GitHub Pages","Documentation","Static Site Generator","Python"]},{"location":"blog/2023/11/27/hosting-mkdocs-documentation-on-github-pages/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have the following prerequisites in place:</p> <ul> <li>A MkDocs project set up on your local machine.</li> <li>A GitHub account where you can create a new repository.</li> </ul>","tags":["MkDocs","GitHub Pages","Documentation","Static Site Generator","Python"]},{"location":"blog/2023/11/27/hosting-mkdocs-documentation-on-github-pages/#steps","title":"Steps","text":"","tags":["MkDocs","GitHub Pages","Documentation","Static Site Generator","Python"]},{"location":"blog/2023/11/27/hosting-mkdocs-documentation-on-github-pages/#1-create-a-github-repository","title":"1. Create a GitHub Repository","text":"<ol> <li> <p>Go to your GitHub account and log in.</p> </li> <li> <p>Click on the \"New\" button to create a new repository.</p> </li> <li> <p>Enter a name for your repository, choose whether it should be public or private, and configure other repository settings as needed. Then, click \"Create repository.\"</p> </li> </ol>","tags":["MkDocs","GitHub Pages","Documentation","Static Site Generator","Python"]},{"location":"blog/2023/11/27/hosting-mkdocs-documentation-on-github-pages/#2-push-your-mkdocs-project-to-github","title":"2. Push Your MkDocs Project to GitHub","text":"<p>To host your MkDocs documentation on GitHub, you need to push your local project to your GitHub repository. Follow these steps:</p> <p></p><pre><code># Initialize a Git repository in your MkDocs project folder (if not already initialized)\ncd /path/to/your/mkdocs/project\ngit init\n\n# Add all the files to the Git repository and commit them\ngit add .\ngit commit -m \"Initial commit\"\n\n# Link your local Git repository to your GitHub repository (replace placeholders)\ngit remote add origin https://github.com/your-username/your-repo.git\n\n# Push your local repository to GitHub\ngit push -u origin master\n</code></pre> Replace your-username with your GitHub username and your-repo with the name of your GitHub repository.","tags":["MkDocs","GitHub Pages","Documentation","Static Site Generator","Python"]},{"location":"blog/2023/11/27/hosting-mkdocs-documentation-on-github-pages/#3-enable-github-pages","title":"3. Enable GitHub Pages","text":"<p>GitHub Pages allows you to host static websites directly from your repository. To enable GitHub Pages for your MkDocs documentation, follow these steps:</p> <ol> <li>Go to your GitHub repository and click on the \"Settings\" tab.</li> <li>Scroll down to the \"GitHub Pages\" section and click on the \"Source\" dropdown menu.</li> <li>Select \"master branch\" as the source and click \"Save.\"</li> </ol>","tags":["MkDocs","GitHub Pages","Documentation","Static Site Generator","Python"]},{"location":"blog/2023/11/27/hosting-mkdocs-documentation-on-github-pages/#4-access-your-documentation-online","title":"4. Access Your Documentation Online","text":"<p>Once you have enabled GitHub Pages, your MkDocs documentation will be accessible online. To access it, go to the following URL:</p> <p></p><pre><code>https://your-username.github.io/your-repo/\n</code></pre> Replace your-username with your GitHub username and your-repo with the name of your GitHub repository.","tags":["MkDocs","GitHub Pages","Documentation","Static Site Generator","Python"]},{"location":"blog/2023/11/27/hosting-mkdocs-documentation-on-github-pages/#conclusion","title":"Conclusion","text":"<p>Hosting your documentation on GitHub Pages can have certain advantages in terms of accessibility and collaboration, but whether it's \"safer\" than keeping everything on your local device depends on your specific needs and security considerations. Here are some points to consider:</p> <p>Advantages of Hosting on GitHub Pages:</p> <ol> <li> <p>Accessibility: When you host your documentation on GitHub Pages, it becomes accessible online, allowing a wider audience to access it without requiring access to your local device.</p> </li> <li> <p>Version Control: GitHub provides robust version control capabilities. You can track changes, collaborate with others, and easily revert to previous versions if needed.</p> </li> <li> <p>Backup: Your documentation is stored on GitHub's servers, providing a level of backup. Even if your local device experiences issues, your documentation remains safe on GitHub.</p> </li> <li> <p>Collaboration: Hosting on GitHub allows for collaborative editing and contributions from team members or the open-source community.</p> </li> <li> <p>Availability: GitHub Pages offers high availability and uptime, ensuring your documentation is accessible to users around the world.</p> </li> </ol> <p>Security Considerations:</p> <ol> <li> <p>Privacy: Make sure you understand the privacy settings of your GitHub repository. If your documentation contains sensitive information, you should keep it private and limit access.</p> </li> <li> <p>Authentication: Implement strong authentication methods for your GitHub account to prevent unauthorized access.</p> </li> <li> <p>Data Ownership: While GitHub is a reputable platform, consider that your data is hosted on third-party servers. Ensure you retain ownership of your documentation content.</p> </li> <li> <p>Backup Strategy: While GitHub provides backup, it's still a good practice to maintain your own backup of critical documentation on your local device or another secure location.</p> </li> <li> <p>Compliance: If you're subject to specific compliance regulations or security requirements, consult with your organization's IT/security team to ensure compliance when hosting documentation on third-party platforms.</p> </li> </ol> <p>In summary, hosting your documentation on GitHub Pages can enhance accessibility, collaboration, and version control. It can be a safer option for sharing and collaborating on non-sensitive documentation. However, security and privacy considerations should be evaluated, and you should ensure that your data remains secure and compliant with any applicable regulations.</p>","tags":["MkDocs","GitHub Pages","Documentation","Static Site Generator","Python"]},{"location":"blog/2024/01/30/setting-up-your-development-environment-for-dreambot-scripting---intellij-idea/","title":"Setting Up Your Development Environment For DreamBot Scripting - Intellij IDEA","text":"","tags":["DreamBot","Java","Scripting","Intellij IDEA"]},{"location":"blog/2024/01/30/setting-up-your-development-environment-for-dreambot-scripting---intellij-idea/#setting-up-your-development-environment-for-dreambot-scripting-intellij-idea","title":"Setting Up Your Development Environment For DreamBot Scripting: Intellij IDEA","text":"<p>In this tutorial, we'll guide you through the process of setting up your development environment for DreamBot scripting. This setup will enable you to create and execute your own scripts.</p>","tags":["DreamBot","Java","Scripting","Intellij IDEA"]},{"location":"blog/2024/01/30/setting-up-your-development-environment-for-dreambot-scripting---intellij-idea/#prerequisites","title":"Prerequisites","text":"<p>Before beginning, ensure you have:</p> <ol> <li>The Java Development Kit (JDK) installed. Instructions are available in the Installing JDK section.</li> <li>DreamBot installed on your computer. Launch it at least once to access the client files.</li> </ol>","tags":["DreamBot","Java","Scripting","Intellij IDEA"]},{"location":"blog/2024/01/30/setting-up-your-development-environment-for-dreambot-scripting---intellij-idea/#integrated-development-environment-ide","title":"Integrated Development Environment (IDE)","text":"<p>Since DreamBot scripts are written in Java, using an Integrated Development Environment (IDE) like IntelliJ IDEA can be very helpful.</p>","tags":["DreamBot","Java","Scripting","Intellij IDEA"]},{"location":"blog/2024/01/30/setting-up-your-development-environment-for-dreambot-scripting---intellij-idea/#download-and-install-intellij-idea","title":"Download and Install IntelliJ IDEA","text":"<ul> <li>Visit the JetBrains IntelliJ IDEA website and install the latest Community build version.</li> </ul>","tags":["DreamBot","Java","Scripting","Intellij IDEA"]},{"location":"blog/2024/01/30/setting-up-your-development-environment-for-dreambot-scripting---intellij-idea/#create-a-new-project","title":"Create a New Project","text":"<p> 1. Open IntelliJ IDEA. 2. Click New Project. 3. Select Java, with IntelliJ as the build system. 4. Choose the JDK you downloaded earlier. 5. Name your script and set the project's save location. 6. Click Create.</p> <p></p>","tags":["DreamBot","Java","Scripting","Intellij IDEA"]},{"location":"blog/2024/01/30/setting-up-your-development-environment-for-dreambot-scripting---intellij-idea/#configure-the-project","title":"Configure the Project","text":"<ol> <li>Right-click the src folder and choose New -&gt; Java Class.</li> <li>Name your class, e.g., \"TestScript\".</li> </ol>","tags":["DreamBot","Java","Scripting","Intellij IDEA"]},{"location":"blog/2024/01/30/setting-up-your-development-environment-for-dreambot-scripting---intellij-idea/#add-dependencies","title":"Add Dependencies","text":"<ol> <li>Go to File -&gt; Project Structure.</li> <li>Under Libraries, click the \"+\" and select Java.</li> <li>Navigate to the DreamBot BotData folder and choose the <code>client.jar</code> file.</li> </ol>","tags":["DreamBot","Java","Scripting","Intellij IDEA"]},{"location":"blog/2024/01/30/setting-up-your-development-environment-for-dreambot-scripting---intellij-idea/#add-an-artifact","title":"Add an Artifact","text":"<ol> <li>Go to File -&gt; Project Structure.</li> <li>Select Artifacts.</li> <li>Click \"+\" and choose JAR -&gt; From modules with dependencies.</li> <li>Set the Output directory to the DreamBot Scripts folder.<ul> <li>Windows: <code>C:\\Users\\YOUR_USER\\DreamBot\\Scripts</code></li> <li>Linux/MacOS: <code>/home/YOUR_USER/DreamBot/Scripts</code></li> </ul> </li> <li>Exclude <code>client.jar</code> from the artifact by removing it from the list.</li> </ol> <p>For detailed instructions on script setup and execution, refer to the Running Your First Script guide.</p>","tags":["DreamBot","Java","Scripting","Intellij IDEA"]},{"location":"blog/2024/01/30/setting-up-your-development-environment-for-dreambot-scripting---intellij-idea/#summary-and-expense-overview","title":"Summary and Expense Overview","text":"<p>Utilizing RAG and Langchain with GPT-4 for this blog post has been enlightening. The RAG AI Assistant has been invaluable in formulating ideas and providing project assistance. Below is the cost breakdown for using RAG AI Assistant:</p> <ul> <li>Total Tokens Processed: 1797</li> <li>Tokens for Prompts: 1285</li> <li>Tokens for Completions: 512</li> <li>Overall Expenditure (USD): $0.06927</li> </ul> <p>This highlights the efficiency and cost-effectiveness of the RAG AI Assistant in content creation.</p>","tags":["DreamBot","Java","Scripting","Intellij IDEA"]},{"location":"blog/2023/11/06/available-voices-from-elevenlabs-api-in-november-2023/","title":"Available Voices from ElevenLabs API in November 2023","text":"","tags":["API","Text-to-Speech","ElevenLabs","Voices","Narration","Video Games"]},{"location":"blog/2023/11/06/available-voices-from-elevenlabs-api-in-november-2023/#voice-profiles","title":"Voice Profiles","text":"<p>ElevenLabs API offers a diverse range of voices, perfect for various use cases like narration and video game character voices. The following voice profiles are available as of November 2023:</p>","tags":["API","Text-to-Speech","ElevenLabs","Voices","Narration","Video Games"]},{"location":"blog/2023/11/06/available-voices-from-elevenlabs-api-in-november-2023/#rachel-the-articulate-narrator","title":"Rachel: The Articulate Narrator","text":"<p>Rachel's voice carries a balance of clarity and tranquility, ideal for narration and audiobook projects.</p> <ul> <li>Voice ID: <pre><code>21m00Tcm4TlvDq8ikWAM\n</code></pre></li> <li>Accent: American</li> <li>Description: Calm</li> <li>Age: Young</li> <li>Gender: Female</li> <li>Use Case: Narration</li> <li>Preview Voice</li> </ul>","tags":["API","Text-to-Speech","ElevenLabs","Voices","Narration","Video Games"]},{"location":"blog/2023/11/06/available-voices-from-elevenlabs-api-in-november-2023/#clyde-the-veteran-storyteller","title":"Clyde: The Veteran Storyteller","text":"<p>Clyde offers a voice rich with experience, perfect for gritty narratives or character roles that demand a seasoned tone.</p> <ul> <li>Voice ID: <pre><code>2EiwWnXFnvU5JabPnv8n\n</code></pre></li> <li>Accent: American</li> <li>Description: War veteran</li> <li>Age: Middle-aged</li> <li>Gender: Male</li> <li>Use Case: Video games</li> <li>Preview Voice</li> </ul>","tags":["API","Text-to-Speech","ElevenLabs","Voices","Narration","Video Games"]},{"location":"blog/2023/11/06/available-voices-from-elevenlabs-api-in-november-2023/#domi-the-confident-influencer","title":"Domi: The Confident Influencer","text":"<p>Domi's commanding voice is filled with confidence, suited for strong narrative leads or powerful corporate presentations.</p> <ul> <li>Voice ID: <pre><code>AZnzlk1XvdvUeBnXmlld\n</code></pre></li> <li>Accent: American</li> <li>Description: Strong</li> <li>Age: Young</li> <li>Gender: Female</li> <li>Use Case: Narration</li> <li>Preview Voice</li> </ul>","tags":["API","Text-to-Speech","ElevenLabs","Voices","Narration","Video Games"]},{"location":"blog/2023/11/06/available-voices-from-elevenlabs-api-in-november-2023/#dave-the-engaging-entertainer","title":"Dave: The Engaging Entertainer","text":"<p>Dave's British-Essex accent adds a unique and engaging flavor, ideal for interactive content and characters with a touch of humor.</p> <ul> <li>Voice ID: <pre><code>CYw3kZ02Hs0563khs1Fj\n</code></pre></li> <li>Accent: British-Essex</li> <li>Description: Conversational</li> <li>Age: Young</li> <li>Gender: Male</li> <li>Use Case: Video games</li> <li>Preview Voice</li> </ul>","tags":["API","Text-to-Speech","ElevenLabs","Voices","Narration","Video Games"]},{"location":"blog/2023/11/06/available-voices-from-elevenlabs-api-in-november-2023/#fin-the-rugged-sea-captain","title":"Fin: The Rugged Sea Captain","text":"<p>Fin's voice, with its Irish accent and seasoned timbre, is perfectly suited for characters with depth and a storied past.</p> <ul> <li>Voice ID: <pre><code>D38z5RcWu1voky8WS1ja\n</code></pre></li> <li>Accent: Irish</li> <li>Description: Sailor</li> <li>Age: Old</li> <li>Gender: Male</li> <li>Use Case: Video games</li> <li>Preview Voice</li> </ul>","tags":["API","Text-to-Speech","ElevenLabs","Voices","Narration","Video Games"]},{"location":"blog/2024/02/04/setting-up-runelite-for-building-with-intellij-idea/","title":"Setting Up RuneLite for Building with IntelliJ IDEA","text":"<p>Setting up RuneLite for building with IntelliJ IDEA involves several steps. Here's a step-by-step guide to get you started:</p>","tags":["RuneLite","IntelliJ IDEA","JDK 11","Maven"]},{"location":"blog/2024/02/04/setting-up-runelite-for-building-with-intellij-idea/#getting-started","title":"Getting Started","text":"<ol> <li> <p>Download and Install IntelliJ IDEA: If you haven't already, download and install IntelliJ IDEA. The Community Edition is free and sufficient for RuneLite development.</p> </li> <li> <p>Install JDK 11: RuneLite is built using JDK 11. You can install this JDK version through IntelliJ IDEA itself by selecting the Eclipse Temurin (AdoptOpenJDK HotSpot) version 11 during the setup.</p> </li> </ol>","tags":["RuneLite","IntelliJ IDEA","JDK 11","Maven"]},{"location":"blog/2024/02/04/setting-up-runelite-for-building-with-intellij-idea/#importing-the-project","title":"Importing the Project","text":"<ol> <li> <p>Clone RuneLite Repository: Open IntelliJ IDEA and select <code>Check out from Version Control</code> &gt; <code>Git</code>. Then, in the URL field, enter RuneLite's repository URL: <code>https://github.com/runelite/runelite</code>. If you plan to contribute, fork the repository on GitHub and clone your fork instead.</p> </li> <li> <p>Open the Project: After cloning, IntelliJ IDEA will ask if you want to open the project. Confirm by clicking <code>Yes</code>.</p> </li> </ol>","tags":["RuneLite","IntelliJ IDEA","JDK 11","Maven"]},{"location":"blog/2024/02/04/setting-up-runelite-for-building-with-intellij-idea/#installing-lombok","title":"Installing Lombok","text":"<ol> <li>Install Lombok Plugin: RuneLite uses Lombok, which requires a plugin in IntelliJ IDEA.</li> <li>Go to <code>File</code> &gt; <code>Settings</code> (on macOS <code>IntelliJ IDEA</code> &gt; <code>Preferences</code>) &gt; <code>Plugins</code>.</li> <li>In the Marketplace tab, search for <code>Lombok</code> and install the plugin.</li> <li>Restart IntelliJ IDEA after installation.</li> </ol>","tags":["RuneLite","IntelliJ IDEA","JDK 11","Maven"]},{"location":"blog/2024/02/04/setting-up-runelite-for-building-with-intellij-idea/#building-the-project","title":"Building the Project","text":"<ol> <li>Build with Maven: RuneLite uses Maven for dependency management and building.</li> <li>Locate the <code>Maven</code> tab on the right side of IntelliJ IDEA.</li> <li>Expand the <code>RuneLite (root)</code> project, navigate to <code>Lifecycle</code>, and double-click <code>install</code>.</li> <li>After building, click the refresh icon in the Maven tab to ensure IntelliJ IDEA picks up the changes.</li> </ol>","tags":["RuneLite","IntelliJ IDEA","JDK 11","Maven"]},{"location":"blog/2024/02/04/setting-up-runelite-for-building-with-intellij-idea/#running-the-project","title":"Running the Project","text":"<ol> <li>Run RuneLite:</li> <li>In the <code>Project</code> tab on the left, navigate to <code>runelite -&gt; runelite-client -&gt; src -&gt; main -&gt; java -&gt; net -&gt; runelite -&gt; client</code>.</li> <li>Right-click the <code>RuneLite</code> class and select <code>Run 'RuneLite.main()'</code>.</li> </ol>","tags":["RuneLite","IntelliJ IDEA","JDK 11","Maven"]},{"location":"blog/2024/02/04/setting-up-runelite-for-building-with-intellij-idea/#conclusion","title":"Conclusion","text":"<p>You've now set up and run RuneLite using IntelliJ IDEA! If you encounter any issues, consult the <code>Troubleshooting</code> section of the RuneLite wiki for common solutions. Remember to keep both your JDK and IntelliJ IDEA up to date to avoid potential issues.</p>","tags":["RuneLite","IntelliJ IDEA","JDK 11","Maven"]},{"location":"blog/2023/12/03/productivity-tools-in-2024/","title":"Productivity Tools in 2024","text":"","tags":["Productivity","GitHub Copilot","Evernote","Raindrop.io","Google Calendar"]},{"location":"blog/2023/12/03/productivity-tools-in-2024/#productivity-tools-in-2024","title":"Productivity Tools in 2024","text":"","tags":["Productivity","GitHub Copilot","Evernote","Raindrop.io","Google Calendar"]},{"location":"blog/2023/12/03/productivity-tools-in-2024/#notetaking-and-task-management","title":"Notetaking and Task Management","text":"<p>In my attempt to cut down on subscriptions in 2024, I'll be switching to Microsoft Visual Studio Code with GitHub Copilot as my go-to AI assistant in helping me churn out more content for my blog and YouTube channel. </p> <p>I'll be switching to a productivity toolset consisting of Evernote with Kanbanote, Anki, Raindrop.io, and Google Calendar. I want to be more note-focused than ever with data-hungry Large Language Models (LLMs) becoming more of a norm.</p> <p>I've gone through my personal Apple subscriptions and canceled all of them, these are separate from my shared family subscriptions such as Chaupal, a Punjabi, Bhojpuri, and Haryanvi video streaming service. I've also canceled my MidJourney and ChatGPT subscriptions. I intend on using fewer applications so I can utilize the most of what I have and if I do start using a new subscription service I'll be sure to buy residential Turkish proxies to get the best price whilst keeping my running total of subscriptions to a minimum.</p> <p>Accordingly, some other subscription services I need to check Turkish pricing for are:</p> <ul> <li> ElevanLabs</li> <li> Grammarly</li> <li> Dropbox</li> </ul> <p>To sum up my 2024 productivity stack:</p> <ul> <li> Microsoft Visual Studio Code</li> <li> GitHub Copilot</li> <li> Evernote</li> <li> Kanbanote</li> <li> Raindrop.io</li> <li> Google Calendar</li> </ul> <p>Useful links:</p> <ol> <li>IP Burger for Turkish residential proxies</li> <li>Prepaid Credit Card for Turkish subscriptions</li> </ol>","tags":["Productivity","GitHub Copilot","Evernote","Raindrop.io","Google Calendar"]},{"location":"blog/2023/12/03/productivity-tools-in-2024/#microsoft-visual-studio-code","title":"Microsoft Visual Studio Code","text":"<p>Microsoft Visual Studio Code is a free source-code editor made by Microsoft for Windows, Linux, and macOS. </p>","tags":["Productivity","GitHub Copilot","Evernote","Raindrop.io","Google Calendar"]},{"location":"blog/2023/12/03/productivity-tools-in-2024/#password-manager","title":"Password Manager","text":"","tags":["Productivity","GitHub Copilot","Evernote","Raindrop.io","Google Calendar"]},{"location":"blog/2023/12/03/productivity-tools-in-2024/#roboform","title":"RoboForm","text":"<p>RoboForm is a password manager and form filler tool that automates password entering and form filling, developed by Siber Systems, Inc. It is available for many web browsers, as a downloadable application, and as a mobile application. RoboForm stores web passwords on its servers, and offers to synchronize passwords between multiple computers and mobile devices. RoboForm offers a Family Plan for up to 5 users which I share with my family.</p>","tags":["Productivity","GitHub Copilot","Evernote","Raindrop.io","Google Calendar"]},{"location":"blog/2023/12/03/productivity-tools-in-2024/#theme","title":"Theme","text":"<p>Dracula Theme is a dark theme for programs Alacritty, Alfred, Atom, BetterDiscord, Emacs, Firefox, Gnome Terminal, Google Chrome, Hyper, Insomnia, iTerm, JetBrains IDEs, Notepad++, Slack, Sublime Text, Terminal.app, Vim, Visual Studio, Visual Studio Code, Windows Terminal, and Xcode. </p> <p>With it's easy-on-the-eyes color scheme, Dracula Theme is on my list of must-have themes for any application I use.</p>","tags":["Productivity","GitHub Copilot","Evernote","Raindrop.io","Google Calendar"]},{"location":"blog/2023/11/15/nvidias-nemotron-3-8b-chat-steerlm---empowering-conversational-ai-with-stateful-text-generation/","title":"NVIDIA's Nemotron-3-8B-Chat-SteerLM - Empowering Conversational AI with Stateful Text Generation","text":"","tags":["NVIDIA","Nemotron-3-8B-Chat-SteerLM","AI","Language Model","Transformer","Stateful Text Generation","OpenAI","GPT"]},{"location":"blog/2023/11/15/nvidias-nemotron-3-8b-chat-steerlm---empowering-conversational-ai-with-stateful-text-generation/#nvidias-nemotron-3-8b-chat-steerlm-empowering-conversational-ai-with-stateful-text-generation","title":"NVIDIA's Nemotron-3-8B-Chat-SteerLM: Empowering Conversational AI with Stateful Text Generation","text":"","tags":["NVIDIA","Nemotron-3-8B-Chat-SteerLM","AI","Language Model","Transformer","Stateful Text Generation","OpenAI","GPT"]},{"location":"blog/2023/11/15/nvidias-nemotron-3-8b-chat-steerlm---empowering-conversational-ai-with-stateful-text-generation/#introduction","title":"Introduction","text":"<p>In the world of AI, language models have taken center stage for their ability to generate human-like text responses to a wide range of queries. NVIDIA's Nemotron-3-8B-Chat-SteerLM is one such model, offering a powerful tool for generative AI creators working on conversational AI models. Let's dive into the details of this model and understand how it works, its intended use, potential risks, and its unique feature of remembering previous answers.</p>","tags":["NVIDIA","Nemotron-3-8B-Chat-SteerLM","AI","Language Model","Transformer","Stateful Text Generation","OpenAI","GPT"]},{"location":"blog/2023/11/15/nvidias-nemotron-3-8b-chat-steerlm---empowering-conversational-ai-with-stateful-text-generation/#model-overview","title":"Model Overview","text":"<p>Nemotron-3-8B-Chat-SteerLM is an 8 billion-parameter generative language model based on the Nemotron-3-8B base model. It boasts customizability through the SteerLM method, allowing users to control model outputs dynamically during inference. This model is designed to generate text responses and code, making it a versatile choice for a range of applications.</p>","tags":["NVIDIA","Nemotron-3-8B-Chat-SteerLM","AI","Language Model","Transformer","Stateful Text Generation","OpenAI","GPT"]},{"location":"blog/2023/11/15/nvidias-nemotron-3-8b-chat-steerlm---empowering-conversational-ai-with-stateful-text-generation/#intended-application-domain","title":"Intended Application &amp; Domain","text":"<p>This model is tailored for text-to-text generation, where it takes text input and generates text output. Its primary purpose is to assist generative AI creators in the development of conversational AI models. Whether it's chatbots, virtual assistants, or customer support systems, this model excels in generating text-based responses to user queries.</p>","tags":["NVIDIA","Nemotron-3-8B-Chat-SteerLM","AI","Language Model","Transformer","Stateful Text Generation","OpenAI","GPT"]},{"location":"blog/2023/11/15/nvidias-nemotron-3-8b-chat-steerlm---empowering-conversational-ai-with-stateful-text-generation/#model-type","title":"Model Type","text":"<p>Nemotron-3-8B-Chat-SteerLM belongs to the Transformer architecture family, renowned for its effectiveness in natural language processing tasks. Its architecture enables it to understand and generate human-like text.</p>","tags":["NVIDIA","Nemotron-3-8B-Chat-SteerLM","AI","Language Model","Transformer","Stateful Text Generation","OpenAI","GPT"]},{"location":"blog/2023/11/15/nvidias-nemotron-3-8b-chat-steerlm---empowering-conversational-ai-with-stateful-text-generation/#intended-user","title":"Intended User","text":"<p>Developers and data scientists are the primary users of this model. They can leverage it to create conversational AI models that generate coherent and contextually relevant text responses in a conversational context.</p>","tags":["NVIDIA","Nemotron-3-8B-Chat-SteerLM","AI","Language Model","Transformer","Stateful Text Generation","OpenAI","GPT"]},{"location":"blog/2023/11/15/nvidias-nemotron-3-8b-chat-steerlm---empowering-conversational-ai-with-stateful-text-generation/#stateful-text-generation","title":"Stateful Text Generation","text":"<p>One of the standout features of this model is its statefulness. It has the ability to remember previous answers in a conversation. This capability allows it to maintain context and generate responses that are not just coherent but also contextually relevant. For example, in a multi-turn conversation, it can refer back to previous responses to ensure continuity and relevancy.</p>","tags":["NVIDIA","Nemotron-3-8B-Chat-SteerLM","AI","Language Model","Transformer","Stateful Text Generation","OpenAI","GPT"]},{"location":"blog/2023/11/15/nvidias-nemotron-3-8b-chat-steerlm---empowering-conversational-ai-with-stateful-text-generation/#how-the-model-works","title":"How the Model Works","text":"<p>Nemotron-3-8B-Chat-SteerLM is a large language model that operates by generating text and code in response to prompts. Users input a text prompt, and the model utilizes its pre-trained knowledge to craft a text-based response. The stateful nature of the model means that it can remember and consider the conversation history, enabling it to generate contextually appropriate responses. This feature enhances the conversational quality of the AI, making interactions feel more natural and meaningful.</p>","tags":["NVIDIA","Nemotron-3-8B-Chat-SteerLM","AI","Language Model","Transformer","Stateful Text Generation","OpenAI","GPT"]},{"location":"blog/2023/11/15/nvidias-nemotron-3-8b-chat-steerlm---empowering-conversational-ai-with-stateful-text-generation/#performance-metrics","title":"Performance Metrics","text":"<p>The model's performance is evaluated based on two critical metrics:</p> <ol> <li> <p>Throughput: This metric measures how many requests the model can handle within a given time frame. It is essential for assessing the model's efficiency in real-world production environments.</p> </li> <li> <p>Latency: Latency gauges the time taken by the model to respond to a single request. Lower latency is desirable, indicating quicker responses and smoother user experiences.</p> </li> </ol>","tags":["NVIDIA","Nemotron-3-8B-Chat-SteerLM","AI","Language Model","Transformer","Stateful Text Generation","OpenAI","GPT"]},{"location":"blog/2023/11/15/nvidias-nemotron-3-8b-chat-steerlm---empowering-conversational-ai-with-stateful-text-generation/#potential-known-risks","title":"Potential Known Risks","text":"<p>It's crucial to be aware of potential risks when using Nemotron-3-8B-Chat-SteerLM:</p> <ul> <li> <p>Bias and Toxicity: The model was trained on data from the internet, which may contain toxic language and societal biases. Consequently, it may generate responses that amplify these biases and return toxic or offensive content, especially when prompted with toxic inputs.</p> </li> <li> <p>Accuracy and Relevance: The model may generate answers that are inaccurate, omit key information, or include irrelevant or redundant text. This can lead to socially unacceptable or undesirable text, even if the input prompt itself is not offensive.</p> </li> </ul>","tags":["NVIDIA","Nemotron-3-8B-Chat-SteerLM","AI","Language Model","Transformer","Stateful Text Generation","OpenAI","GPT"]},{"location":"blog/2023/11/15/nvidias-nemotron-3-8b-chat-steerlm---empowering-conversational-ai-with-stateful-text-generation/#licensing","title":"Licensing","text":"<p>The use of this model is governed by the \"NVIDIA AI Foundation Models Community License Agreement.\" Users must adhere to the terms and conditions outlined in the agreement when utilizing the model.</p>","tags":["NVIDIA","Nemotron-3-8B-Chat-SteerLM","AI","Language Model","Transformer","Stateful Text Generation","OpenAI","GPT"]},{"location":"blog/2023/11/15/nvidias-nemotron-3-8b-chat-steerlm---empowering-conversational-ai-with-stateful-text-generation/#conclusion","title":"Conclusion","text":"<p>NVIDIA's Nemotron-3-8B-Chat-SteerLM represents a significant advancement in generative AI for conversational applications. With its stateful text generation capability and Transformer architecture, it offers a versatile solution for developers and data scientists working in this domain. However, it's important to be mindful of potential biases and accuracy issues, as well as adhere to the licensing terms when utilizing this powerful AI tool.</p>","tags":["NVIDIA","Nemotron-3-8B-Chat-SteerLM","AI","Language Model","Transformer","Stateful Text Generation","OpenAI","GPT"]},{"location":"blog/2023/11/21/transferring-files-between-wsl-and-windows/","title":"Transferring Files Between WSL and Windows","text":"","tags":["WSL","Windows","Ubuntu","Linux","File Transfer","SCP","Secure Copy"]},{"location":"blog/2023/11/21/transferring-files-between-wsl-and-windows/#transferring-files-between-wsl-and-windows","title":"Transferring Files Between WSL and Windows","text":"","tags":["WSL","Windows","Ubuntu","Linux","File Transfer","SCP","Secure Copy"]},{"location":"blog/2023/11/21/transferring-files-between-wsl-and-windows/#logging-into-zomro-vps-using-wsl-in-ubuntu-cli","title":"Logging into Zomro VPS using WSL in Ubuntu CLI","text":"<p>To log into your Zomro VPS using WSL (Windows Subsystem for Linux) in the Ubuntu CLI, you can use the <code>ssh</code> command. Here are the steps to do it:</p> <ul> <li>Open your Ubuntu terminal in WSL. You can do this by searching for \"Ubuntu\" in the Windows Start menu and launching it.</li> <li>In the Ubuntu terminal, use the <code>ssh</code> command to connect to your Zomro VPS. Replace <code>your_username</code> with your actual username and <code>your_server_ip</code> with the IP address of your Zomro VPS:</li> </ul> <pre><code>ssh your_username@your_server_ip\n</code></pre> <p>For example, if your username is \"root\" and your server's IP address is \"123.456.789.0,\" the command would be:</p> <pre><code>ssh root@123.456.789.0\n</code></pre> <ul> <li>Press Enter after entering the command. You will be prompted to enter your password for the VPS.</li> <li>After entering the correct password, you should be logged into your Zomro VPS via SSH. You will see a command prompt for your VPS, and you can start running commands on the remote server.</li> </ul> <p></p> <p>That's it! You have successfully logged into your Zomro VPS using WSL's Ubuntu CLI. You can now manage your server and perform various tasks as needed.</p>","tags":["WSL","Windows","Ubuntu","Linux","File Transfer","SCP","Secure Copy"]},{"location":"blog/2023/11/21/transferring-files-between-wsl-and-windows/#locating-file-paths-in-ubuntu-cli","title":"Locating File Paths in Ubuntu CLI","text":"<p>In Ubuntu CLI (Command Line Interface), it's essential to know how to find the file paths of directories and files. This knowledge allows you to navigate your file system effectively and reference files for various tasks. Here are some useful commands and techniques for locating file paths:</p>","tags":["WSL","Windows","Ubuntu","Linux","File Transfer","SCP","Secure Copy"]},{"location":"blog/2023/11/21/transferring-files-between-wsl-and-windows/#present-working-directory-pwd","title":"Present Working Directory (pwd)","text":"<p>The <code>pwd</code> command stands for \"Present Working Directory\" and displays the absolute path of your current location within the file system. Simply enter the following command:</p> <pre><code>pwd\n</code></pre> <p>The terminal will respond with the absolute path to your current directory, helping you understand where you are in the file system.</p>","tags":["WSL","Windows","Ubuntu","Linux","File Transfer","SCP","Secure Copy"]},{"location":"blog/2023/11/21/transferring-files-between-wsl-and-windows/#listing-directory-contents-ls","title":"Listing Directory Contents (ls)","text":"<p>The <code>ls</code> command is used to list the contents of a directory. When executed without any arguments, it displays the files and subdirectories in your current directory. For example:</p> <pre><code>ls\n</code></pre> <p>This command will list the files and directories in your current location.</p>","tags":["WSL","Windows","Ubuntu","Linux","File Transfer","SCP","Secure Copy"]},{"location":"blog/2023/11/21/transferring-files-between-wsl-and-windows/#finding-a-file-find","title":"Finding a File (find)","text":"<p>If you need to locate a specific file within your file system, you can use the <code>find</code> command. Specify the starting directory and the filename you're looking for. For example, to find a file named \"example.txt\" starting from the root directory, use:</p> <pre><code>find / -name example.txt\n</code></pre> <p>This command will search the entire file system for \"example.txt\" and display its path if found.</p>","tags":["WSL","Windows","Ubuntu","Linux","File Transfer","SCP","Secure Copy"]},{"location":"blog/2023/11/21/transferring-files-between-wsl-and-windows/#navigating-directories-cd","title":"Navigating Directories (cd)","text":"<p>The <code>cd</code> command allows you to change directories and move through the file system. You can use it to navigate to specific locations. For instance, to move to a directory named \"documents,\" use:</p> <pre><code>cd documents\n</code></pre> <p>You can also use relative paths, such as <code>cd ..</code> to go up one level or <code>cd /path/to/directory</code> to specify an absolute path.</p>","tags":["WSL","Windows","Ubuntu","Linux","File Transfer","SCP","Secure Copy"]},{"location":"blog/2023/11/21/transferring-files-between-wsl-and-windows/#file-explorer-integration","title":"File Explorer Integration","text":"<p>In many cases, you can easily locate file paths by using a graphical file explorer like Windows File Explorer. WSL allows you to access your Windows files and directories under the <code>/mnt</code> directory. For example, your Windows <code>C:</code> drive is typically accessible at <code>/mnt/c/</code>.</p> <p>Understanding how to locate file paths in Ubuntu CLI is crucial for efficient file management and navigation. These commands and techniques will empower you to work effectively with your files and directories.</p>","tags":["WSL","Windows","Ubuntu","Linux","File Transfer","SCP","Secure Copy"]},{"location":"blog/2023/11/21/transferring-files-between-wsl-and-windows/#transferring-files-from-wsl-to-windows","title":"Transferring Files from WSL to Windows","text":"<p>Transferring files from your WSL (Windows Subsystem for Linux) environment to your Windows system is a common task and can be done using several methods. I\u2019ll be discussing the Secure Copy method in this tutorial.</p>","tags":["WSL","Windows","Ubuntu","Linux","File Transfer","SCP","Secure Copy"]},{"location":"blog/2023/11/21/transferring-files-between-wsl-and-windows/#using-scp-secure-copy","title":"Using SCP (Secure Copy)","text":"<p>You can transfer files from WSL using the <code>scp</code> (Secure Copy) command. Here's the syntax:</p> <pre><code>scp username@WindowsIP:/path/to/source/file /path/to/destination/in/WSL/\n</code></pre> <ul> <li><code>username</code>: Your Windows username.</li> <li><code>WindowsIP</code>: The IP address or hostname of your Windows system.</li> <li><code>/path/to/source/file</code>: The path to the file in your Windows file system that you want to copy.</li> <li><code>/path/to/destination/in/WSL/</code>: The destination path in your WSL environment.</li> </ul> <p>For example, to copy all files located in <code>/root/zomro-selenium-base/screenshots/*</code>  to your Windows Desktop, you can use:</p> <pre><code>scp root@45.88.107.136:/root/zomro-selenium-base/screenshots/* \"/mnt/c/Users/Harminder Nijjar/Desktop/\"\n</code></pre> <p>This command will copy all files in <code>/root/zomro-selenium-base/screenshots/</code> to your Windows Desktop. Make sure to adjust the source and destination paths as needed for your specific use case.</p>","tags":["WSL","Windows","Ubuntu","Linux","File Transfer","SCP","Secure Copy"]},{"location":"blog/2023/11/21/transferring-files-between-wsl-and-windows/#conclusion","title":"Conclusion","text":"<p>Transferring files between WSL and Windows is a common operation and can be accomplished using the Secure Copy (SCP) command. Whether you need to copy files from WSL to Windows or from Windows to WSL, SCP provides a secure and efficient</p>","tags":["WSL","Windows","Ubuntu","Linux","File Transfer","SCP","Secure Copy"]},{"location":"css/css/","title":"css","text":""},{"location":"projects/2024/current_projects/","title":"Harminder's Current Projects","text":"Home Automation <p>Projects involving Home Assistant, Eufy Security, and Valetudo.</p> Self-Hosting <p>Managing services like SNIPE-IT, osTicket, Plex, Jellyfin and GameVault on self-hosted platforms.</p> 3D Printing <p>Using QIDI Tech Plus4, QIDI Tech X-Plus3, and Autodesk Fusion for various 3D printing projects.</p>","tags":["home automation","self-hosting","3D printing"]},{"location":"projects/archived/2023/current_projects/","title":"Harminder's Current Projects","text":"Dubbing Pipeline <p>Optimizing voiceover workflows for content creators and viewers.</p> Midjourney Automation Bot <p>Automating Midjourney image generation and social media posting.</p> RunescapeGPT <p>A Runescape ChatGPT Bot.</p> Social Media Automation <p>Automating social media posting and engagement.</p> Websites <p>Building and maintaining websites with modern technologies.</p>","tags":["machine learning","web development","object detection","knowledge base"]},{"location":"projects/archived/2023/dubbing_pipeline/dub_with_elevenlabs/","title":"Dub with ElevenLabs Google Chrome Extension","text":"","tags":["ElevenLabs","Dubbing","AI","Chrome Extension","YouTube"]},{"location":"projects/archived/2023/dubbing_pipeline/dub_with_elevenlabs/#dub-with-elevenlabs-google-chrome-extension","title":"Dub with ElevenLabs Google Chrome Extension","text":"<p>At Passivebot, we\u2019re always looking for better and more efficient ways to improve user experience. In line with this goal, we are excited to get working on our latest project: Dub with ElevenLabs Google Chrome Extension. This extension streamlines the experience of viewing dubbed content on YouTube by allowing users to easily switch between dubbed and original audio tracks with a single click.</p>","tags":["ElevenLabs","Dubbing","AI","Chrome Extension","YouTube"]},{"location":"projects/archived/2023/dubbing_pipeline/dub_with_elevenlabs/#dubbing","title":"Dubbing","text":"<p>Dubbing is a new technology but it is rapidly gaining traction and has already been used to create episodes of popular anime shows like Attack on Titan, My Hero Academia, and Boku no Hero Academia. It is the process of replacing one language with another in a movie or TV series. From Hollywood movies released in multiple languages to Japanese anime dubbed in Spanish, dubbing is now commonplace among content streaming services.</p> <p>Dubbing has now become even more accessible thanks to AI dubbing, a technology developed by ElevenLabs. AI dubbing makes it easier for content creators to offer their work in multiple languages by leveraging the power of artificial intelligence. It ensures that the original speaker's voice characteristics are preserved, which is especially important with dubbed content like film and anime.</p>","tags":["ElevenLabs","Dubbing","AI","Chrome Extension","YouTube"]},{"location":"projects/archived/2023/dubbing_pipeline/dub_with_elevenlabs/#dub-with-elevenlabs","title":"Dub with ElevenLabs","text":"<p>Without an official release of a Dubbing API by ElevenLabs, Passivebot independently developed a Chrome extension named \"Dub with \"ElevenLabs .\" This tool facilitates the viewing of AI-dubbed content on YouTube, targeting content-creators who have not yet adopted AI dubbing technology. It simplifies the process for users to experience dubbed video content.</p> <p>Requirements:</p> <ul> <li> Allow users to watch dubbed content on YouTube with a single click</li> </ul> <p>The extension will need to simplify the process of having to enter the YouTube link of the video on ElevenLabs to a single click.</p> <p>We intend to do this by running a background script that will automatically detect if the user is on a YouTube video page. If the user is on a YouTube video page, the extension will display a button that will allow the user to automatically input the YouTube link of the video into the ElevenLabs AI dubbing tool, wait for the video to be dubbed, and then switch the video to the dubbed version by changing the video stream to the link of the dubbed video.</p>","tags":["ElevenLabs","Dubbing","AI","Chrome Extension","YouTube"]},{"location":"projects/archived/2023/dubbing_pipeline/dub_with_elevenlabs/#design","title":"Design","text":"<p>Originally, textension will be designed to be as simple as possible. It will have a single button that will allow the user perform the following actions:</p> <ul> <li> Automatically input the YouTube link of the video into the ElevenLabs AI dubbing tool</li> <li> Wait for the video to be dubbed</li> <li> Switch the video to the dubbed version by changing the video stream to the link of the dubbed video</li> </ul>","tags":["ElevenLabs","Dubbing","AI","Chrome Extension","YouTube"]},{"location":"projects/archived/2023/dubbing_pipeline/dub_with_elevenlabs/#implementation","title":"Implementation","text":"<p>We will be using the following technologies to implement the extension:</p> <ol> <li> JavaScript</li> <li> HTML</li> <li> CSS</li> </ol>","tags":["ElevenLabs","Dubbing","AI","Chrome Extension","YouTube"]},{"location":"projects/archived/2023/dubbing_pipeline/dub_with_elevenlabs/#progress","title":"Progress","text":"","tags":["ElevenLabs","Dubbing","AI","Chrome Extension","YouTube"]},{"location":"projects/archived/2023/dubbing_pipeline/dub_with_elevenlabs/#2023-11-10-1252","title":"2023-11-10 12:52","text":"<p>I've created a repository for the project and added the following files: 1. <code>manifest.json</code>: This file contains the metadata for the extension along with what files to load and what permissions to request. 2. <code>background.js</code>: This file contains the background script that will run in the background and detect if the user is on a YouTube video page. 3. <code>content.js</code>: This file contains the content script that will run on the YouTube video page and display the button to the user.</p> <p>I've been able to successfully load the extension in developer mode and have the background script detect if the user is on a YouTube video page. I've also been able to successfully open a new tab with ElevenLabs and input the YouTube link of the video into the AI dubbing tool. I'm currently working on waiting for the video to be dubbed and then switching the video to the dubbed version by changing the video stream to the link of the dubbed video.</p>","tags":["ElevenLabs","Dubbing","AI","Chrome Extension","YouTube"]},{"location":"projects/archived/2023/dubbing_pipeline/dub_with_elevenlabs/#2023-11-11-2029","title":"2023-11-11 20:29","text":"<p>The project is on pause for my subscription to renew and for new credits to be added to my account. At the current subscription prices, it costs $11 per hour of dubbing using the Creator plan at $22/mo and $8.25 per hour using the Growing Business plan at $330/mo. In the meantime, I'll be look into free alternatives to ElevenLabs.</p>","tags":["ElevenLabs","Dubbing","AI","Chrome Extension","YouTube"]},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/midjourney_automation_bot_overview/","title":"Midjourney Automation Bot","text":"","tags":["Midjourney","Discord","Python"]},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/midjourney_automation_bot_overview/#midjourney-automation-bot","title":"Midjourney Automation Bot","text":"","tags":["Midjourney","Discord","Python"]},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/midjourney_automation_bot_overview/#overview","title":"Overview","text":"<p>The Midjourney Automation Bot is a groundbreaking, open-source project that leverages the capabilities of OpenAI's GPT-3 model for automated image generation. This tool is specifically designed to interact with Discord channels, enabling users to create a wide array of art forms such as illustrations, digital paintings, or sketches through simple text prompts. It stands out for its ease of use, making it a valuable asset for artists, developers, and enthusiasts in the realm of automated image generation.</p>","tags":["Midjourney","Discord","Python"]},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/midjourney_automation_bot_overview/#key-features","title":"Key Features","text":"<ul> <li>Automated Discord Interaction: The bot is programmed to autonomously interact with Discord channels, streamlining the process of sending and receiving image generation commands.</li> <li>User-Defined Prompts: Users can input custom prompts to guide the image generation process, offering a high degree of creative control.</li> <li>GPT-3 Integration: Utilizing OpenAI's GPT-3 model, the bot can interpret prompts and generate corresponding images with remarkable accuracy and creativity.</li> <li>Customizable Upscale Options: The bot includes options for upscaling the generated images, allowing users to enhance image quality according to their needs.</li> <li>User-Friendly Web Interface: Equipped with a web interface, the bot offers an intuitive platform for users to interact with and control its functions.</li> <li>Robust Logging: The bot maintains detailed logs of its operations and any encountered errors, ensuring transparency and ease of troubleshooting.</li> <li>Open Source and Customizable: Released under the MIT License, the bot encourages broad usage and modification, catering to a wide range of applications and user modifications.</li> </ul>","tags":["Midjourney","Discord","Python"]},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/midjourney_automation_bot_overview/#setup-guide","title":"Setup Guide","text":"","tags":["Midjourney","Discord","Python"]},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/midjourney_automation_bot_overview/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<ol> <li>Open your command line interface (CLI).</li> <li>Navigate to the directory where you want to clone the repository.</li> <li>Run the following command: <pre><code>   git clone https://github.com/passivebot/midjourney-automation-bot.git\n</code></pre></li> <li>This will create a copy of the repository on your local machine.</li> </ol>","tags":["Midjourney","Discord","Python"]},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/midjourney_automation_bot_overview/#step-2-install-dependencies","title":"Step 2: Install Dependencies","text":"<ol> <li>Ensure you have Python and pip installed on your machine. If not, download and install Python from python.org. Pip is included automatically.</li> <li>Navigate to the directory of the cloned repository in your CLI. Use the <code>cd</code> command to change directories. For example:    <code>bash    cd midjourney-automation-bot</code></li> <li>Once in the directory, run the following command to install the required dependencies: <pre><code>   pip install -r requirements.txt\n</code></pre></li> <li>This command will install all the Python packages listed in the <code>requirements.txt</code> file.</li> </ol>","tags":["Midjourney","Discord","Python"]},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/midjourney_automation_bot_overview/#step-3-set-the-openai-api-key-in-the-environment-variable","title":"Step 3: Set the OpenAI API Key in the Environment Variable","text":"<ol> <li>You need to have an OpenAI API key. If you don't have one, you can obtain it from the OpenAI API portal.</li> <li> <p>Setting the environment variable depends on your operating system.</p> </li> <li> <p>For Windows:</p> <ol> <li>Open Command Prompt or PowerShell.</li> <li>Run the following command (replace Your_API_Key with your actual API key):  <code>bash setx OPENAI_API_KEY \"Your_API_Key\"</code></li> <li>Restart your CLI to apply the changes.</li> </ol> </li> <li> <p>For macOS/Linux:</p> <ol> <li>Open Terminal.</li> <li>Add the export command to your shell profile file (like .bashrc, .zshrc, etc.). For example, if you're using bash, you can run:     <pre><code>echo 'export OPENAI_API_KEY=\"Your_API_Key\"' &gt;&gt; ~/.bashrc\n</code></pre></li> <li>Replace Your_API_Key with your actual API key.</li> <li>Apply the changes by running source ~/.bashrc (or the respective file for your shell).</li> </ol> </li> </ol>","tags":["Midjourney","Discord","Python"]},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/midjourney_automation_bot_overview/#usage-guide","title":"Usage Guide","text":"","tags":["Midjourney","Discord","Python"]},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/midjourney_automation_bot_overview/#step-1-open-discord","title":"Step 1: Open Discord","text":"<ol> <li>Open your browser and go to Discord.</li> <li>Log in to your Discord account.</li> <li>Navigate to the channel where you've added the Midjourney Discord bot too and copy the URL of the channel.</li> </ol>","tags":["Midjourney","Discord","Python"]},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/midjourney_automation_bot_overview/#step-2-start-the-bot","title":"Step 2: Start the Bot","text":"<ol> <li>Open your CLI and navigate to the directory of the cloned repository.</li> <li>Run the following command:    python main.py</li> <li>This will start the bot and open a new browser window with the GUI.</li> <li>Enter the required details including the Discord channel URL, the bot command, the art type, the descriptors, and the topic.</li> <li>Click on the Start Bot button to start the bot.</li> <li>The bot will now start generating ten images in the specified channel, upscale them, and download them to your local machine.</li> </ol>","tags":["Midjourney","Discord","Python"]},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/features/requested_features/automated_instagram_image_uploading_with_advanced_captioning/","title":"Automated Instagram image uploading with advanced captioning","text":"","tags":["Midjourney","Instagram","GPT-4V","AI","Python"]},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/features/requested_features/automated_instagram_image_uploading_with_advanced_captioning/#feature-request-auto-posting-images-on-instagram-with-enhanced-customization","title":"Feature Request: Auto Posting Images on Instagram with Enhanced Customization","text":"","tags":["Midjourney","Instagram","GPT-4V","AI","Python"]},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/features/requested_features/automated_instagram_image_uploading_with_advanced_captioning/#description","title":"Description","text":"<p>This feature includes dynamically generated captions using GPT-4V and enables the bot to upload random albums or individual images to Instagram. The captions are generated based on specific prompts, and the images are selected randomly from specified directories.</p>","tags":["Midjourney","Instagram","GPT-4V","AI","Python"]},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/features/requested_features/automated_instagram_image_uploading_with_advanced_captioning/#priority","title":"Priority","text":"<ul> <li>High Priority: This feature is essential for automating social media content and enhancing user engagement. It can save time and effort for content creators and marketers.</li> </ul>","tags":["Midjourney","Instagram","GPT-4V","AI","Python"]},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/features/requested_features/automated_instagram_image_uploading_with_advanced_captioning/#implementation-plan","title":"Implementation Plan","text":"","tags":["Midjourney","Instagram","GPT-4V","AI","Python"]},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/features/requested_features/automated_instagram_image_uploading_with_advanced_captioning/#1-caption-generation","title":"1. Caption Generation","text":"<ul> <li>Objective: Generate professional and formal captions using OpenAI's GPT-3 model.</li> <li>Method: Utilize OpenAI's API to create captions based on specific prompts related to the images.</li> <li>Dependencies: OpenAI API key, Python 3.9.4.</li> </ul>","tags":["Midjourney","Instagram","GPT-4V","AI","Python"]},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/features/requested_features/automated_instagram_image_uploading_with_advanced_captioning/#2-image-selection-and-processing","title":"2. Image Selection and Processing","text":"<ul> <li>Objective: Select random albums or individual images from specified folders.</li> <li>Method: Use Python's <code>os</code> and <code>random</code> libraries to choose random folders or files. Convert PNG images to JPG using the PIL library.</li> <li>Dependencies: Python's <code>os</code>, <code>random</code>, and <code>PIL</code> libraries.</li> </ul>","tags":["Midjourney","Instagram","GPT-4V","AI","Python"]},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/features/requested_features/automated_instagram_image_uploading_with_advanced_captioning/#3-instagram-interaction","title":"3. Instagram Interaction","text":"<ul> <li>Objective: Log in to Instagram and upload the selected images with generated captions.</li> <li>Method: Use the <code>instagrapi</code> library to log in, select images, and upload them with captions.</li> <li>Dependencies: <code>instagrapi</code> library, Instagram credentials.</li> </ul>","tags":["Midjourney","Instagram","GPT-4V","AI","Python"]},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/features/requested_features/automated_instagram_image_uploading_with_advanced_captioning/#4-error-handling-and-cleanup","title":"4. Error Handling and Cleanup","text":"<ul> <li>Objective: Handle exceptions and clean up local files after uploading.</li> <li>Method: Implement try-except blocks to catch errors and remove local files after successful uploads.</li> <li>Dependencies: Python's <code>os</code> library.</li> </ul>","tags":["Midjourney","Instagram","GPT-4V","AI","Python"]},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/features/requested_features/automated_instagram_image_uploading_with_advanced_captioning/#technical-considerations","title":"Technical Considerations","text":"<ul> <li>Language: Python 3.9.4.</li> <li>Libraries: <code>openai</code>, <code>instagrapi</code>, <code>PIL</code>, <code>os</code>, <code>random</code>.</li> <li>Environment Variables: OpenAI API key, Instagram username, and password.</li> <li>Compatibility: Ensure compatibility with different image formats and Instagram's API changes.</li> <li>Security: Securely handle credentials and API keys.</li> <li>Testing: Implement unit tests to validate each component of the feature.</li> <li>Documentation: Provide detailed documentation for setup, usage, and troubleshooting.</li> </ul>","tags":["Midjourney","Instagram","GPT-4V","AI","Python"]},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/features/requested_features/automated_instagram_image_uploading_with_advanced_captioning/#conclusion","title":"Conclusion","text":"<p>The \"Auto Posting Images on Instagram with Enhanced Customization\" feature is a high-priority enhancement that can significantly streamline social media management. The implementation plan outlines a systematic approach to developing this feature, considering various technical aspects and dependencies. Proper testing, security measures, and comprehensive documentation will be vital for the successful deployment and user adoption of this feature.</p>","tags":["Midjourney","Instagram","GPT-4V","AI","Python"]},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/","title":"Improved Documentation","text":""},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/#feature-request-improving-documentation","title":"Feature Request: Improving Documentation","text":""},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/#description","title":"Description","text":"<p>This feature focuses on enhancing the existing documentation of the project, making it more comprehensive, user-friendly, and accessible. It includes updating the current guides, adding new tutorials, and providing clear examples for better understanding.</p>"},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/#priority","title":"Priority","text":"<ul> <li>Medium Priority: Improved documentation is vital for both new users and experienced developers, facilitating easier navigation and understanding of the system.</li> </ul>"},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/#implementation-plan","title":"Implementation Plan","text":""},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/#1-assessing-current-documentation","title":"1. Assessing Current Documentation","text":"<ul> <li>Objective: Identify gaps, outdated information, and areas that need clarification in the existing documentation.</li> <li>Method: Review the current documentation, gather feedback from users, and create a list of areas for improvement.</li> </ul>"},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/#2-updating-existing-guides","title":"2. Updating Existing Guides","text":"<ul> <li>Objective: Revise and update existing guides to reflect the latest changes and best practices.</li> <li>Method: Collaborate with developers to ensure that the documentation aligns with the current codebase and functionalities.</li> </ul>"},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/#3-creating-new-tutorials-and-examples","title":"3. Creating New Tutorials and Examples","text":"<ul> <li>Objective: Provide step-by-step tutorials and clear examples to assist users in understanding how to use the system.</li> <li>Method: Work with experienced users and developers to create practical examples and tutorials.</li> </ul>"},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/#4-enhancing-accessibility","title":"4. Enhancing Accessibility","text":"<ul> <li>Objective: Make documentation accessible to a broader audience, including non-technical users.</li> <li>Method: Use simple language, add visual aids, and ensure that the documentation is available in different formats.</li> </ul>"},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/#technical-considerations","title":"Technical Considerations","text":"<ul> <li>Language: English, with potential translations into other languages.</li> <li>Tools: Markdown, documentation generators like Sphinx or Jekyll.</li> <li>Compatibility: Ensure that the documentation is viewable on different devices and browsers.</li> <li>Testing: Regularly review and update the documentation to keep it current.</li> <li>Collaboration: Engage with the community for feedback and contributions.</li> </ul>"},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/#timeline","title":"Timeline","text":"<p>-[ ] Week 1-2: Assessing current documentation and identifying areas for improvement. - [ ] Week 3-4: Updating existing guides and aligning them with the latest changes. -[ ] Week 5-6: Creating new tutorials and examples. -[ ] Week 7-8: Enhancing accessibility and final review.</p>"},{"location":"projects/archived/2023/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/#conclusion","title":"Conclusion","text":"<p>Improved documentation is essential for the growth and sustainability of the project. It not only helps new users to get started quickly but also supports experienced developers in understanding the more complex aspects of the system. A systematic approach, collaboration with the community, and regular updates will ensure that the documentation remains an invaluable resource for all users.</p>"},{"location":"projects/archived/2023/runescapegpt/wiki/runescapegpt_overview/","title":"RunescapeGPT - A Runescape ChatGPT Bot","text":"","tags":["RunescapeGPT","Runescape","Python","RAG","Retrieval-Augmented Generation","OpenAI","GPT","AI","Language Model","Transformer"]},{"location":"projects/archived/2023/runescapegpt/wiki/runescapegpt_overview/#an-overview-of-runescapegpt-enhancing-bot-functionality-through-advanced-ai-techniques-and-community-driven-data","title":"An Overview of RunescapeGPT: Enhancing Bot Functionality through Advanced AI Techniques and Community-Driven Data","text":"<p>RunescapeGPT emerges as an exemplary innovation in the domain of gaming automation for Old School RuneScape (OSRS), characterized by its integration of Retrieval-Augmented Generation (RAG) and the utilization of datasets derived from the DreamBot community forums. This sophisticated system represents the zenith of collaborative intelligence, channeling the collective expertise of gamers to refine and elevate the operational efficiency of gaming bots.</p>","tags":["RunescapeGPT","Runescape","Python","RAG","Retrieval-Augmented Generation","OpenAI","GPT","AI","Language Model","Transformer"]},{"location":"projects/archived/2023/runescapegpt/wiki/runescapegpt_overview/#leveraging-collective-insight","title":"Leveraging Collective Insight","text":"<p>The compendium of knowledge contained within the DreamBot forums encapsulates a diverse range of discussions, strategies, and technical expertise from dedicated OSRS enthusiasts. By assimilating this repository of shared knowledge, RunescapeGPT transcends traditional bot capabilities, employing the nuanced wisdom of seasoned players to enhance its algorithmic performance.</p>","tags":["RunescapeGPT","Runescape","Python","RAG","Retrieval-Augmented Generation","OpenAI","GPT","AI","Language Model","Transformer"]},{"location":"projects/archived/2023/runescapegpt/wiki/runescapegpt_overview/#the-strategic-advantage-of-retrieval-augmented-generation","title":"The Strategic Advantage of Retrieval-Augmented Generation","text":"<p>RAG represents a paradigm shift in machine learning, merging the generative prowess of language models with robust information retrieval systems. In the context of RunescapeGPT, this technology strategically extracts pertinent information from the DreamBot forums, which then informs and refines the bot\u2019s code generation processes. Such a methodology ensures that the bot\u2019s actions are not only contextually appropriate but also reflective of the community\u2019s best practices.</p>","tags":["RunescapeGPT","Runescape","Python","RAG","Retrieval-Augmented Generation","OpenAI","GPT","AI","Language Model","Transformer"]},{"location":"projects/archived/2023/runescapegpt/wiki/runescapegpt_overview/#capabilities-of-runescapegpt","title":"Capabilities of RunescapeGPT","text":"<ul> <li>Advanced Adaptive Learning: The bot exhibits an aptitude for learning from both the direct outcomes of its actions and the indirect input of aggregated community data, fostering a progressively intelligent gameplay experience.</li> <li>Refined Dynamic Code Generation: RunescapeGPT\u2019s application of RAG enables it to dynamically generate codes that are attuned to the fluctuating scenarios within the game, yielding a more effective and strategic bot functionality.</li> <li>Community-Informed Development: By harnessing the collective intelligence of the DreamBot forums, RunescapeGPT benefits from a continuous influx of contemporary strategies and solutions, thus embodying a bot that is shaped by the community it serves.</li> <li>Commitment to Ethical Standards and Regulatory Compliance: The development of RunescapeGPT is meticulously aligned with ethical AI deployment practices, ensuring adherence to OSRS's terms of service and community conduct codes.</li> </ul>","tags":["RunescapeGPT","Runescape","Python","RAG","Retrieval-Augmented Generation","OpenAI","GPT","AI","Language Model","Transformer"]},{"location":"projects/archived/2023/runescapegpt/wiki/runescapegpt_overview/#conclusion","title":"Conclusion","text":"<p>The inception of RunescapeGPT marks a significant advancement in gaming automation technology. By leveraging RAG and drawing upon the extensive datasets provided by the DreamBot forums, RunescapeGPT is not merely an automation tool but a learning entity that incrementally improves through collective gaming intelligence. This advancement not only solidifies RunescapeGPT's position as a leading-edge bot but also exemplifies the potential for AI to evolve in concert with player communities, setting a new benchmark for intelligent automation within the gaming milieu.</p>","tags":["RunescapeGPT","Runescape","Python","RAG","Retrieval-Augmented Generation","OpenAI","GPT","AI","Language Model","Transformer"]},{"location":"projects/archived/2023/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/","title":"Unfollow Non-Followers on Instagram Using Instagrapi","text":"","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/#unfollow-non-followers-on-instagram-using-instagrapi","title":"Unfollow Non-Followers on Instagram Using Instagrapi","text":"<p>Disclaimer: Before proceeding, it's important to note that using unofficial APIs can lead to potential violations of Instagram's terms of service. Such actions may result in restrictions on your account, up to and including a permanent ban. The information provided here is for educational purposes, and I strongly advise against misusing it in any way.</p>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/#introduction","title":"Introduction","text":"<p>In the world of Instagram, engagement and follower count are often seen as measures of popularity and influence. It's not uncommon for users to seek a balanced follower-to-following ratio. If you're looking to clean up your Instagram following list and unfollow users who do not follow you back, Instagrapi\u2014a powerful, unofficial Instagram API wrapper for Python\u2014can help automate this process.</p>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/#code","title":"Code","text":"","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/#install-instagrapi","title":"Install Instagrapi","text":"<p>First and foremost, Instagrapi needs to be installed in your Python environment. It can be installed easily using pip, Python's package installer. Simply run the following command in your terminal or command prompt:</p> <pre><code>pip install instagrapi\n</code></pre> <p>This command downloads and installs the Instagrapi library, making its functionality available for your Python scripts.</p>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/#create-an-instagrapi-client","title":"Create an Instagrapi Client","text":"<p>To interact with Instagram, you must create an instance of the Instagrapi client. This is your gateway to performing API calls:</p> <pre><code>from instagrapi import Client\n\nclient = Client()\n</code></pre> <p>This snippet imports the <code>Client</code> class from the Instagrapi package and instantiates it.</p>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/#login-to-instagram","title":"Login to Instagram","text":"<p>Next, you'll need to authenticate with Instagram. Replace \"username\" and \"password\" with your actual Instagram credentials:</p> <pre><code>client.login(\"username\", \"password\")\n</code></pre> <p>The <code>login</code> method signs you into Instagram, allowing your script to act on your behalf. Remember to keep your credentials secure and never share them with anyone.</p>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/#get-your-followers-and-following","title":"Get Your Followers and Following","text":"<p>Now, retrieve the list of user IDs for both your followers and the accounts you follow:</p> <pre><code>followers = client.user_followers(client.user_id)\nfollowing = client.user_following(client.user_id)\n</code></pre> <p>These methods return a dictionary of users who are your followers and users you are following, respectively.</p>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/#identify-users-to-unfollow","title":"Identify Users to Unfollow","text":"<p>With both lists at your disposal, you can now identify which users you follow that do not follow you back:</p> <pre><code>users_to_unfollow = [user for user in following if user not in followers]\n</code></pre> <p>This line of code uses a list comprehension to create a new list of user IDs that represents people you follow who aren't following you back.</p>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/#unfollow-users","title":"Unfollow Users","text":"<p>Once you have the list of non-followers, you can loop through it and unfollow each user:</p> <pre><code>for user_to_unfollow in users_to_unfollow:\n    client.user_unfollow(user_to_unfollow)\n</code></pre> <p>This loop calls the <code>user_unfollow</code> method for each user in the <code>users_to_unfollow</code> list, effectively cleaning up your following list.</p>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/#logout-optional","title":"Logout (Optional)","text":"<p>After the script has completed its task, you may choose to log out of the client session:</p> <pre><code>client.logout()\n</code></pre> <p>Logging out is a good practice to end the session, especially when running the script periodically.</p>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/#conclusion","title":"Conclusion","text":"<p>This script serves as a tool for managing your social media presence more effectively especially if you have a large following. It can be run periodically to keep your following list clean and balanced.</p>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/","title":"Downloading Media","text":"","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#downloading-media","title":"Downloading Media","text":"","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#introduction","title":"Introduction","text":"<p>In this tutorial, we will be covering how to download all media from a specific Instagram profile along with captions and hashtags, then store this data in a SQLite database. This will allow us to store the data we scrape from Instagram to later be uploaded to Truth Social. We will be using the following tools:</p> <ul> <li>Instagrapi</li> <li>SQLite</li> </ul>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#project-description","title":"Project Description","text":"<p>Due to the recent actions of mainstream technology giants against figures like Alex Jones and then-President Donald Trump, alternative digital ecosystems have grown in popularity. Platforms like Truth Social endorse unrestricted speech, attracting users looking for alternatives to the mainstream. By utilizing tools like Instagrapi and SQLite, we can store the data we scrape from Instagram to later be uploaded to Truth Social. </p>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#project-objectives","title":"Project Objectives","text":"<ul> <li> Download, at minimum, 100 media from a specific Instagram profile along with captions and hashtags, then store this data in a SQLite database.</li> </ul>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#deadline","title":"Deadline","text":"<ul> <li> Achieve the project goals by 2023-10-13.</li> </ul>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#choosing-a-platform","title":"Choosing a Platform","text":"","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#truth-social","title":"Truth Social","text":"<p>Based on the Google Trends data, Truth Social currently leads in popularity among alternative platforms. This indicates a potentially large user base, making it the ideal choice for profile replication.</p>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#selecting-a-profile-to-replicate","title":"Selecting a Profile to Replicate","text":"","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#nicole-wolf","title":"Nicole Wolf","text":"<p>Our initial step involves the careful selection of a profile to replicate. In the context of this tutorial, we have chosen to replicate the profile of a well-known Instagram influencer, Nicole Wolf, a professional web developer and dancer, known as @joeel56. With over 176,000 followers and a public profile, Nicole's online presence is significant. Given that her German background aligns with the demographics of Truth Social, this profile is ideal for replication.</p> <p></p>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#creating-a-sqlite-database","title":"Creating a SQLite Database","text":"","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#installation","title":"Installation","text":"<p>SQLite is a relational database management system that is embedded into the Python programming language. This database management system is well-documented and can be accessed here. We will be using this database management system to store the data we scrape from Instagram. The following sections will outline the steps required to create a SQLite database.</p> <p>Install the \"sqlite3\" library if you haven't already. You can install it using pip:</p> <pre><code>pip install sqlite3\n</code></pre>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#database-initialization","title":"Database Initialization","text":"<p>After installing the \"sqlite3\" library, we will need to create a SQLite database. This database will store the data we scrape from Instagram. To create a SQLite database, we will need to import the <code>sqlite3</code> library. We will then need to initialize a connection to this database. This will create a SQLite database that we can use to store the data we scrape from Instagram.</p> <pre><code>import sqlite3\n\n# Create a connection to the SQLite database\nconn = sqlite3.connect(\"instagram_data.db\")\n</code></pre>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#creating-a-table-for-media-data","title":"Creating a Table for Media Data","text":"<p>After creating a SQLite database, we will need to create a table in this database. This table will store the data we scrape from Instagram. To create a table in a SQLite database, we will need to create a cursor object. We will then need to execute a SQL query to create a table in our SQLite database. This will create a table in our SQLite database that we can use to store the data we scrape from Instagram.</p> <pre><code># Define cursor\n\ncursor = conn.cursor()\n\n# Establish a table structure\n\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS media_data (\nid INTEGER PRIMARY KEY,\nmedia BLOB NOT NULL,\ncaption TEXT,\nhashtags TEXT\n)\n''')\n\n# Commit changes and close connection\n\nconn.commit()\nconn.close()\n</code></pre>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#replicating-an-instagram-profile-with-instagrapi","title":"Replicating an Instagram Profile with Instagrapi","text":"","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#instagrapi-installation","title":"Instagrapi Installation","text":"<p>Instagrapi is a Python library that can be used to programmatically interact with Instagram. This library is well-documented and can be accessed here . We will be using this library to replicate our Instagram profile. The following sections will outline the steps required to replicate our Instagram profile.</p> <p>Install the \"instagrapi\" library if you haven't already. You can install it using pip:</p> <pre><code>pip install instagrapi\n</code></pre>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#creating-an-instagram-session","title":"Creating an Instagram Session","text":"<p>To download all media from a specific Instagram profile along with captions and hashtags, then store this data in a Notion database, we will first need to create an Instagram session. This session will allow us to programmatically interact with Instagram. To create an Instagram session, we will need to import the <code>Client</code> class from the <code>instagrapi</code> library. We will then need to create an instance of this class and pass in our Instagram username and password. This will create an Instagram session that we can use to programmatically interact with Instagram.</p> <pre><code>from instagrapi import Client\n\n# Initialize the client\nclient = Client()\n</code></pre>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#authenticating-an-instagram-session","title":"Authenticating an Instagram Session","text":"<p>After creating an Instagram session, we will need to authenticate this session. This will allow us to programmatically interact with Instagram. To authenticate an Instagram session, we will need to call the <code>login</code> method on our Instagram session and pass in our Instagram username and password. This will authenticate our Instagram session and allow us to programmatically interact with Instagram.</p> <pre><code># Authenticate the client\nclient.login(\"username\", \"password\")\n</code></pre>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#downloading-media-from-an-instagram-profile","title":"Downloading Media from an Instagram Profile","text":"<p>After downloading all media from a specific Instagram profile, we will need to store this data in a SQLite database. This will allow us to store the data we scrape from Instagram. To store this data in a SQLite database, we will need to create a connection to our SQLite database. We will then need to create a cursor object. We will then need to execute a SQL query to insert this data into our SQLite database. This will store this data in our SQLite database and allow us to store the data we scrape from Instagram.</p> <pre><code>def download_media_from_profile(username, max_count=100):\n    user_info = cl.user_info_by_username(username)\n    user_id = user_info.pk\n    # Get the user's media\n    medias = cl.user_medias(user_id, amount=max_count)\n    for media in medias:\n        media_id = media.pk\n        caption = media.caption_text  # Get the caption\n        hashtags = [tag.strip(\"#\") for tag in caption.split() if tag.startswith(\"#\")]  # Extract hashtags\n        hashtags = \", \".join(hashtags)  # Convert hashtags to a string\n        # Initialize media_blob as None to handle unexpected media types\n        media_blob = None\n        # Download the media (photo or video) and convert it to binary\n        if media.media_type == 1:  # Photo\n            path = cl.photo_download(media_id, folder=f\"{os.getcwd()}\")\n            media_blob = convert_to_binary(path)\n        elif media.media_type == 2:  # Video\n            path = cl.video_download(media_id, folder=f\"{os.getcwd()}\")\n            media_blob = convert_to_binary(path)\n        # Check if the media_blob is already in the database\n        cursor.execute(\"SELECT id FROM media_data WHERE media = ?\", (media_blob,))\n        existing_media = cursor.fetchone()\n        # If the media doesn't exist in the database, insert it\n        if not existing_media and media_blob:\n            cursor.execute(\"INSERT INTO media_data (media, caption, hashtags) VALUES (?, ?, ?)\", (media_blob, caption, hashtags))\n            conn.commit()\n            # Optionally, remove the downloaded file to save space\n            os.remove(path)\n            logger.info(f\"Downloaded media with caption: {caption}\")\n            logger.info(f\"Hashtags: {hashtags}\\n\")\n        elif existing_media:\n            logger.info(f\"Media already exists in the database. Skipping download for media with caption: {caption}\")\n</code></pre>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#closing-an-instagram-session","title":"Closing an Instagram Session","text":"<p>After downloading all media from a specific Instagram profile, we will need to close this Instagram session. This will allow us to store the data we scrape from Instagram. To close an Instagram session, we will need to call the <code>logout</code> method on our Instagram session. This will close our Instagram session and allow us to store the data we scrape from Instagram.</p> <pre><code># Close the client\nclient.logout()\n</code></pre>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#conclusion","title":"Conclusion","text":"<p>By following the above steps, one can efficiently download all media from a specific Instagram profile along with captions and hashtags, then store this data in a SQLite database. This will allow one to store the data we scrape from Instagram to later be uploaded to Truth Social. </p>","tags":["Instagram","Instagrapi","Python","Social Media","Automation"]},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/uploading_media/uploading_media/","title":"Uploading Media","text":""},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/uploading_media/uploading_media/#uploading-media","title":"Uploading Media","text":""},{"location":"projects/archived/2023/social_media_automation/truth_social/account_cloning/uploading_media/uploading_media/#introduction","title":"Introduction","text":"<p>I am currently working on this section. Please check back later.</p>"},{"location":"projects/archived/2023/web_scraping/web_scraping_overview/","title":"Harnessing Facebook Marketplace Data with Python Scrapers","text":"","tags":["Web Scraping","Facebook Marketplace","Python","Data Analysis"]},{"location":"projects/archived/2023/web_scraping/web_scraping_overview/#harnessing-facebook-marketplace-data-with-python-scrapers","title":"Harnessing Facebook Marketplace Data with Python Scrapers","text":"","tags":["Web Scraping","Facebook Marketplace","Python","Data Analysis"]},{"location":"projects/archived/2023/web_scraping/web_scraping_overview/#overview","title":"Overview","text":"<p>In the vast and competitive world of e-commerce, Facebook Marketplace stands out as a treasure trove of consumer data. To tap into this resource, I've developed three Python-based web scrapers, each tailored to navigate and extract valuable marketplace insights with precision and efficiency. This overview provides a glimpse into their mechanics and the philosophy behind their creation.</p>","tags":["Web Scraping","Facebook Marketplace","Python","Data Analysis"]},{"location":"projects/archived/2023/web_scraping/web_scraping_overview/#key-features","title":"Key Features","text":"<ul> <li>Diverse Data Collection: The scrapers are designed to collect various data points, including product titles, prices, and locations, allowing for a comprehensive market analysis.</li> <li>Adaptability: Each tool has unique features suited for different scraping needs, from simple data extraction to handling dynamic content and large-scale operations.</li> <li>Efficient Automation: The scrapers automate the tedious task of data collection, enabling users to focus on data analysis and application.</li> <li>Ethical Considerations: These tools adhere to best practices in web scraping, ensuring they operate within the legal and ethical boundaries set by the target website.</li> </ul>","tags":["Web Scraping","Facebook Marketplace","Python","Data Analysis"]},{"location":"projects/archived/2023/web_scraping/web_scraping_overview/#comparative-analysis","title":"Comparative Analysis","text":"","tags":["Web Scraping","Facebook Marketplace","Python","Data Analysis"]},{"location":"projects/archived/2023/web_scraping/web_scraping_overview/#comprehensive-python-scraper","title":"Comprehensive Python Scraper","text":"<p>The first scraper is a full-fledged Python script that uses libraries like requests and BeautifulSoup to parse HTML content and sqlite3 to manage data storage. It is the most thorough tool among the three, designed for depth and detail in data collection.</p>","tags":["Web Scraping","Facebook Marketplace","Python","Data Analysis"]},{"location":"projects/archived/2023/web_scraping/web_scraping_overview/#proxy-service-scraper","title":"Proxy Service Scraper","text":"<p>The second scraper operates through proxy services, demonstrating a lightweight and efficient approach to bypass common web scraping barriers. It is especially useful for quick data retrieval across different geographic locations</p>","tags":["Web Scraping","Facebook Marketplace","Python","Data Analysis"]},{"location":"projects/archived/2023/web_scraping/web_scraping_overview/#asynchronous-scraper","title":"Asynchronous Scraper","text":"<p>The third scraper leverages asyncio and playwright, showcasing the power of asynchronous programming to perform multiple requests simultaneously. It is adept at dealing with JavaScript-heavy pages and is indicative of the future of web scraping.</p>","tags":["Web Scraping","Facebook Marketplace","Python","Data Analysis"]},{"location":"projects/archived/2023/web_scraping/web_scraping_overview/#setup-guide","title":"Setup Guide","text":"","tags":["Web Scraping","Facebook Marketplace","Python","Data Analysis"]},{"location":"projects/archived/2023/web_scraping/web_scraping_overview/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python installation</li> <li>Pip for managing Python packages</li> <li>An understanding of command line operations</li> <li>Basic knowledge of HTML and web technologies</li> </ul>","tags":["Web Scraping","Facebook Marketplace","Python","Data Analysis"]},{"location":"projects/archived/2023/web_scraping/web_scraping_overview/#installation","title":"Installation","text":"<ol> <li>Clone the relevant repository.</li> <li>Install the required dependencies via pip.</li> <li>Configure necessary environmental variables, such as API keys.</li> </ol>","tags":["Web Scraping","Facebook Marketplace","Python","Data Analysis"]},{"location":"projects/archived/2023/web_scraping/web_scraping_overview/#usage-guide","title":"Usage Guide","text":"<ul> <li>Navigate to the desired directory.</li> <li>Run the scraper using the Python command.</li> <li>Monitor the output and logs for data and potential errors.</li> </ul> <p>In conclusion, these three scrapers offer a window into the potential of web scraping for e-commerce platforms like Facebook Marketplace. With careful design and ethical usage, they serve as powerful tools for businesses and analysts to gain an edge in the ever-changing market landscape.</p>","tags":["Web Scraping","Facebook Marketplace","Python","Data Analysis"]},{"location":"projects/archived/2023/web_scraping/facebook/facebook_marketplace/bright_data_facebook_marketplace_scraper/","title":"Bright Data Facebook Marketplace Scraper","text":"<p>TODO</p>"},{"location":"projects/archived/2023/web_scraping/facebook/facebook_marketplace/oxylabs_facebook_marketplace_scraper/","title":"Oxylabs Facebook Marketplace Scraper","text":"<p>TODO</p>"},{"location":"projects/archived/2023/web_scraping/facebook/facebook_marketplace/smartproxy_facebook_marketplace_scraper/","title":"Smartproxy Facebook Marketplace Scraper","text":"<p>TODO</p>"},{"location":"projects/archived/2023/websites/mkdocs/knowledge_base/adding_assets/additional_css/tailwind_css/setup_material_mkdocs_insiders_tailwind/","title":"Integrating Tailwind CSS with Material for MkDocs Insiders","text":"","tags":["Tailwind CSS","Material for MkDocs","MkDocs","CSS","Web Development"]},{"location":"projects/archived/2023/websites/mkdocs/knowledge_base/adding_assets/additional_css/tailwind_css/setup_material_mkdocs_insiders_tailwind/#integrating-tailwind-css-with-material-for-mkdocs-insiders","title":"Integrating Tailwind CSS with Material for MkDocs Insiders","text":"","tags":["Tailwind CSS","Material for MkDocs","MkDocs","CSS","Web Development"]},{"location":"projects/archived/2023/websites/mkdocs/knowledge_base/adding_assets/additional_css/tailwind_css/setup_material_mkdocs_insiders_tailwind/#introduction","title":"Introduction","text":"<p>Combine the power of Tailwind CSS with Material for MkDocs Insiders to create a documentation site that not only has the robust features and beautiful design of Material but also the utility-first flexibility and customization of Tailwind CSS. This guide will walk you through the steps to integrate Tailwind CSS into your MkDocs project, allowing you to tailor the look and feel of your site with ease.</p>","tags":["Tailwind CSS","Material for MkDocs","MkDocs","CSS","Web Development"]},{"location":"projects/archived/2023/websites/mkdocs/knowledge_base/adding_assets/additional_css/tailwind_css/setup_material_mkdocs_insiders_tailwind/#setting-up-material-for-mkdocs","title":"Setting Up Material for MkDocs","text":"<p>Start by setting up Material for MkDocs as per the official guidelines. Ensure you have a running MkDocs environment before proceeding with the integration of Tailwind CSS.</p>","tags":["Tailwind CSS","Material for MkDocs","MkDocs","CSS","Web Development"]},{"location":"projects/archived/2023/websites/mkdocs/knowledge_base/adding_assets/additional_css/tailwind_css/setup_material_mkdocs_insiders_tailwind/#install-tailwind-css","title":"Install Tailwind CSS","text":"<p>Within your MkDocs project directory, run the following command to install the necessary packages:</p> <pre><code>npm install tailwindcss postcss autoprefixer\n</code></pre> <p>These packages include Tailwind CSS for the utility-first CSS framework, PostCSS for processing CSS with JavaScript, and Autoprefixer for handling CSS vendor prefixes.</p>","tags":["Tailwind CSS","Material for MkDocs","MkDocs","CSS","Web Development"]},{"location":"projects/archived/2023/websites/mkdocs/knowledge_base/adding_assets/additional_css/tailwind_css/setup_material_mkdocs_insiders_tailwind/#setting-up-postcss-and-tailwind","title":"Setting up PostCSS and Tailwind","text":"<p>Create a <code>postcss.config.js</code> file in the root of your MkDocs project with the following content:</p> <pre><code>module.exports = {\n  plugins: [require('tailwindcss'), require('autoprefixer')]\n}\n</code></pre> <p>Then, initialize your Tailwind configuration file which Tailwind uses to read your customization settings:</p> <pre><code>npx tailwindcss init\n</code></pre>","tags":["Tailwind CSS","Material for MkDocs","MkDocs","CSS","Web Development"]},{"location":"projects/archived/2023/websites/mkdocs/knowledge_base/adding_assets/additional_css/tailwind_css/setup_material_mkdocs_insiders_tailwind/#create-your-custom-css-file","title":"Create Your Custom CSS File","text":"<p>Make a new CSS file in the <code>docs/stylesheets/</code> directory named <code>tailwind.css</code>. This file will import Tailwind's layers for you to use throughout your project:</p> <pre><code>@import 'tailwindcss/base';\n@import 'tailwindcss/components';\n@import 'tailwindcss/utilities';\n</code></pre>","tags":["Tailwind CSS","Material for MkDocs","MkDocs","CSS","Web Development"]},{"location":"projects/archived/2023/websites/mkdocs/knowledge_base/adding_assets/additional_css/tailwind_css/setup_material_mkdocs_insiders_tailwind/#build-tailwind-css","title":"Build Tailwind CSS","text":"<p>Compile your CSS with Tailwind's styles by running:</p> <pre><code>npx tailwindcss build docs/stylesheets/tailwind.css -o docs/stylesheets/output.css\n</code></pre> <p>This command will process your custom CSS file and output a fully compiled CSS file with all of Tailwind's utility classes.</p>","tags":["Tailwind CSS","Material for MkDocs","MkDocs","CSS","Web Development"]},{"location":"projects/archived/2023/websites/mkdocs/knowledge_base/adding_assets/additional_css/tailwind_css/setup_material_mkdocs_insiders_tailwind/#update-mkdocs-configuration","title":"Update MkDocs Configuration","text":"<p>Include the compiled CSS in your MkDocs configuration by editing <code>mkdocs.yml</code>:</p> <pre><code>extra_css:\n  - stylesheets/output.css\n</code></pre>","tags":["Tailwind CSS","Material for MkDocs","MkDocs","CSS","Web Development"]},{"location":"projects/archived/2023/websites/mkdocs/knowledge_base/adding_assets/additional_css/tailwind_css/setup_material_mkdocs_insiders_tailwind/#customization","title":"Customization","text":"<p>You can now use Tailwind CSS classes in your Markdown content or in any HTML templates you're using within your MkDocs site.</p>","tags":["Tailwind CSS","Material for MkDocs","MkDocs","CSS","Web Development"]},{"location":"projects/archived/2023/websites/mkdocs/knowledge_base/adding_assets/additional_css/tailwind_css/setup_material_mkdocs_insiders_tailwind/#build-mkdocs","title":"Build MkDocs","text":"<p>To see the changes take effect, build your MkDocs project with:</p> <pre><code>mkdocs build\n</code></pre> <p>Note: Any changes made to Tailwind or your custom CSS require you to rebuild Tailwind CSS and then rebuild your MkDocs project.</p>","tags":["Tailwind CSS","Material for MkDocs","MkDocs","CSS","Web Development"]},{"location":"projects/archived/2023/websites/vercel/gitbute/3d_github_contributions/3d_github_contributions/","title":"Rendering 3D GitHub Contributions on AWS using Blender and AWS Lambda","text":"","tags":["AWS","AWS Lambda","Blender","3D Rendering","Python","Cloud Computing"]},{"location":"projects/archived/2023/websites/vercel/gitbute/3d_github_contributions/3d_github_contributions/#rendering-3d-github-contributions-on-aws","title":"Rendering 3D GitHub Contributions on AWS","text":"","tags":["AWS","AWS Lambda","Blender","3D Rendering","Python","Cloud Computing"]},{"location":"projects/archived/2023/websites/vercel/gitbute/3d_github_contributions/3d_github_contributions/#introduction","title":"Introduction","text":"<p>In the rapidly evolving landscape of cloud computing and 3D rendering, innovative solutions are constantly being developed to handle complex tasks more efficiently. One such breakthrough is the use of AWS Lambda for rendering 3D scenes, particularly for projects where high scalability and quick turnaround times are crucial. This approach is especially relevant when dealing with a large volume of simpler assets that need to be rendered swiftly, leveraging the power of cloud computing.</p> <p>The concept of running Blender, a popular open-source 3D graphics software, on AWS Lambda, presents a unique blend of flexibility and power. This method is ideal for scenarios where each asset is simple enough to be processed within the constraints of Lambda functions, which, as of my last update, offer up to 6 vCPUs and 10GB of RAM. For more complex rendering tasks, alternatives like EC2 instances or AWS Thinkbox Deadline might be more suitable due to their enhanced computational capabilities.</p> <p>The inspiration for utilizing Blender on AWS Lambda came from discovering that a similar approach was successfully implemented by Theodo in 2021.</p>","tags":["AWS","AWS Lambda","Blender","3D Rendering","Python","Cloud Computing"]},{"location":"projects/archived/2023/websites/vercel/gitbute/3d_github_contributions/3d_github_contributions/#technology-stack","title":"Technology Stack","text":"","tags":["AWS","AWS Lambda","Blender","3D Rendering","Python","Cloud Computing"]},{"location":"projects/archived/2023/websites/vercel/gitbute/3d_github_contributions/3d_github_contributions/#frontend-stack","title":"Frontend Stack","text":"<p>The frontend of this project is developed with Reflex, a Python-based framework ideal for crafting interactive web applications. Reflex's key advantage lies in its ability to enable component reuse across both frontend and backend, crucial for a cohesive user experience. It also offers an extensive range of components for creating a responsive and interactive frontend.</p> <p>Styling is achieved through Tailwind CSS, a utility-first framework that facilitates rapid, custom UI development. This choice is beneficial as it simplifies customization without the need for extensive CSS coding.</p> <p>For hosting, the project utilizes Vercel, a cloud platform optimized for static sites and serverless functions. Vercel's strengths include easy deployment and efficient management of serverless functions, making it a fitting choice for the project's frontend needs. This combination of technologies ensures a streamlined, efficient, and aesthetically pleasing user interface.</p> Language Python Framework Reflex Styling Tailwind CSS Hosting Vercel","tags":["AWS","AWS Lambda","Blender","3D Rendering","Python","Cloud Computing"]},{"location":"projects/archived/2023/websites/vercel/gitbute/3d_github_contributions/3d_github_contributions/#backend-stack","title":"Backend Stack","text":"<p>The project's backend is developed using Reflex, a Python-based framework ideal for building interactive web applications. Reflex's ability to facilitate component reuse across both frontend and backend significantly enhances the user experience. Additionally, it provides a variety of responsive and interactive components for frontend development.</p> <p>For hosting, the backend utilizes AWS Lambda, a serverless computing service. This choice is advantageous due to its scalability and cost-effectiveness, as it only charges for the compute time used. The project also integrates Reflex Database, a NoSQL database optimized for Python, enabling straightforward data storage and retrieval without SQL. Furthermore, AWS S3 is employed for cloud storage, offering efficient data handling capabilities in Python without the need for SQL coding. This combination of technologies ensures a robust and efficient backend infrastructure for the project.</p> Language Python Framework Reflex Hosting AWS Lambda Database Reflex Database Other AWS S3","tags":["AWS","AWS Lambda","Blender","3D Rendering","Python","Cloud Computing"]},{"location":"projects/archived/2023/websites/vercel/gitbute/3d_github_contributions/3d_github_contributions/#online-ide","title":"Online IDE","text":"<p>The online IDE used for this project was Replit, a cloud-based IDE that allows you to code in Python, JavaScript, HTML, CSS, and more. Replit is a great choice for this project because it allows you to easily collaborate with others and share your code with the world.</p> <ul> <li>https://replit.com/@harmindersinghnijjar/Bute-3D-GitHub-Contributions</li> </ul>","tags":["AWS","AWS Lambda","Blender","3D Rendering","Python","Cloud Computing"]},{"location":"projects/archived/2023/websites/vercel/gitbute/3d_github_contributions/3d_github_contributions/#render-pipeline","title":"Render Pipeline","text":"<p>The depicted render pipeline represents a sophisticated, cloud-based 3D rendering process, leveraging AWS services and Blender's capabilities. This pipeline is designed to efficiently handle 3D rendering requests, process them using cloud resources, and deliver the final product back to the user. Below is a detailed description of each step in this pipeline:</p> <ul> <li> <p>User Request Initiation: The process begins with the user initiating a 3D rendering request. This is typically done through an API endpoint, which acts as the interface between the user and the cloud-based rendering system.</p> </li> <li> <p>API Endpoint: Upon receiving a request, the API endpoint acts as a gateway, funneling the user's requirements into the AWS ecosystem. This step is crucial for interpreting the user's request and preparing the system for the subsequent data retrieval process.</p> </li> <li> <p>Data Retrieval: This stage involves fetching the necessary information required for the 3D rendering task. The data retrieval process is a critical component, ensuring that all the required elements, such as textures, models, and other assets, are gathered and ready for processing.</p> </li> <li> <p>Blender on AWS Lambda: Once the data is retrieved, it is sent to Blender, which is hosted on AWS Lambda. AWS Lambda provides a serverless compute service, allowing Blender to run in a highly scalable, event-driven environment. This setup is particularly efficient for handling varying loads and can scale up or down based on the complexity of the rendering tasks.</p> </li> <li> <p>3D Replication Creation: In this phase, Blender processes the data to create a 3D replication. This step involves the actual rendering process where Blender utilizes its powerful 3D graphics tools to generate the desired 3D graphical files.</p> </li> <li> <p>Uploading to AWS S3 Bucket: After the rendering is complete, the 3D graphical files are uploaded to an AWS S3 bucket. S3 provides secure, scalable object storage, making it an ideal choice for storing large files like 3D renders. The use of S3 ensures that the rendered files are stored safely and are easily accessible for download.</p> </li> <li> <p>User Download: Finally, the link to the rendered 3D graphical files in the S3 bucket is sent back to the user. This completes the full cycle of the rendering process, allowing the user to download the final product directly from the cloud.</p> </li> </ul> <p>This full-cycle 3D rendering process on AWS exemplifies a modern, cloud-based approach to 3D graphics production. It highlights the integration of serverless computing, cloud storage, and powerful rendering software to deliver a seamless, scalable, and efficient 3D rendering service.</p>","tags":["AWS","AWS Lambda","Blender","3D Rendering","Python","Cloud Computing"]},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/3d-printing/","title":"3D Printing","text":""},{"location":"blog/category/product-reviews/","title":"Product Reviews","text":""},{"location":"blog/category/safety-concerns/","title":"Safety Concerns","text":""},{"location":"blog/category/community/","title":"Community","text":""},{"location":"blog/category/criminal-justice/","title":"Criminal Justice","text":""},{"location":"blog/category/cybersecurity/","title":"Cybersecurity","text":""},{"location":"blog/category/law-enforcement/","title":"Law Enforcement","text":""},{"location":"blog/category/management/","title":"Management","text":""},{"location":"blog/category/literature-review/","title":"Literature Review","text":""},{"location":"blog/category/academic-writing/","title":"Academic Writing","text":""},{"location":"blog/category/investigative-challenges/","title":"Investigative Challenges","text":""},{"location":"blog/category/project-management/","title":"Project Management","text":""},{"location":"blog/category/time-management/","title":"Time Management","text":""},{"location":"blog/category/planning/","title":"Planning","text":""},{"location":"blog/category/bed-leveling/","title":"Bed Leveling","text":""},{"location":"blog/category/abl/","title":"ABL","text":""},{"location":"blog/category/fluidd/","title":"Fluidd","text":""},{"location":"blog/category/runelite/","title":"RuneLite","text":""},{"location":"blog/category/intellij-idea/","title":"IntelliJ IDEA","text":""},{"location":"blog/category/jdk-11/","title":"JDK 11","text":""},{"location":"blog/category/maven/","title":"Maven","text":""},{"location":"blog/category/dreambot/","title":"DreamBot","text":""},{"location":"blog/category/java/","title":"Java","text":""},{"location":"blog/category/woodcutting/","title":"Woodcutting","text":""},{"location":"blog/category/scripting/","title":"Scripting","text":""},{"location":"blog/category/python/","title":"Python","text":""},{"location":"blog/category/beautifulsoup/","title":"BeautifulSoup","text":""},{"location":"blog/category/selenium/","title":"Selenium","text":""},{"location":"blog/category/computer-vision/","title":"Computer Vision","text":""},{"location":"blog/category/surveillance/","title":"Surveillance","text":""},{"location":"blog/category/dvr/","title":"DVR","text":""},{"location":"blog/category/productivity/","title":"Productivity","text":""},{"location":"blog/category/github-copilot/","title":"GitHub Copilot","text":""},{"location":"blog/category/evernote/","title":"Evernote","text":""},{"location":"blog/category/raindropio/","title":"Raindrop.io","text":""},{"location":"blog/category/google-calendar/","title":"Google Calendar","text":""},{"location":"blog/category/facebook-marketplace-scraper/","title":"Facebook Marketplace Scraper","text":""},{"location":"blog/category/google-sheets-api/","title":"Google Sheets API","text":""},{"location":"blog/category/sqlite/","title":"SQLite","text":""},{"location":"blog/category/telegram-bot-api/","title":"Telegram Bot API","text":""},{"location":"blog/category/smartproxy/","title":"Smartproxy","text":""},{"location":"blog/category/mkdocs/","title":"MkDocs","text":""},{"location":"blog/category/github-pages/","title":"GitHub Pages","text":""},{"location":"blog/category/documentation/","title":"Documentation","text":""},{"location":"blog/category/static-site-generator/","title":"Static Site Generator","text":""},{"location":"blog/category/wsl/","title":"WSL","text":""},{"location":"blog/category/windows/","title":"Windows","text":""},{"location":"blog/category/ubuntu/","title":"Ubuntu","text":""},{"location":"blog/category/linux/","title":"Linux","text":""},{"location":"blog/category/runescape/","title":"Runescape","text":""},{"location":"blog/category/ai/","title":"AI","text":""},{"location":"blog/category/language-model/","title":"Language Model","text":""},{"location":"blog/category/transformer/","title":"Transformer","text":""},{"location":"blog/category/nvidia/","title":"NVIDIA","text":""},{"location":"blog/category/whoosh/","title":"Whoosh","text":""},{"location":"blog/category/indexing/","title":"Indexing","text":""},{"location":"blog/category/linkedin/","title":"LinkedIn","text":""},{"location":"blog/category/skill-assessments/","title":"Skill Assessments","text":""},{"location":"blog/category/quizzes/","title":"Quizzes","text":""},{"location":"blog/category/openai/","title":"OpenAI","text":""},{"location":"blog/category/api/","title":"API","text":""},{"location":"blog/category/text-to-speech/","title":"Text-to-Speech","text":""},{"location":"blog/category/elevenlabs/","title":"ElevenLabs","text":""},{"location":"blog/category/osrs/","title":"OSRS","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"blog/page/3/","title":"Blog","text":""},{"location":"blog/archive/2023/page/2/","title":"2023","text":""}]}